{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07_word_representations.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOs4GZDqbpl+z4cDGN/pFbx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amckenny/text_analytics_intro/blob/main/notebooks/07_word_representations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKHVqnLm4wBC"
      },
      "source": [
        "# Prerequisites\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGl-NniE4U4H"
      },
      "source": [
        "# Install 3rd party packages\n",
        "!pip -q install python-Levenshtein\n",
        "!pip -q install \"gensim==4.0.0\"\n",
        "\n",
        "\n",
        "# Standard library imports\n",
        "import glob\n",
        "from pathlib import Path\n",
        "from itertools import combinations\n",
        "\n",
        "\n",
        "# 3rd party imports\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow.compat.v1 as tf\n",
        "from gensim.models import KeyedVectors\n",
        "from scipy.spatial.distance import cosine\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "# Get GloVe files and load into gensim\n",
        "!mkdir -p GloVe\n",
        "!wget -q https://www.dropbox.com/s/9k39nheab1rhezq/glove.6B.50d.zip?dl=1 -O ./GloVe/glove.6B.50d.zip\n",
        "!unzip -qq -n -d ./GloVe ./GloVe/glove.6B.50d.zip\n",
        "\n",
        "glove_model = KeyedVectors.load_word2vec_format(\"./GloVe/glove.6B.50d.txt\", binary=False, no_header=True)\n",
        "\n",
        "\n",
        "# Load ELMo into tensorflow\n",
        "tf.disable_eager_execution()\n",
        "elmo = hub.Module(\"https://tfhub.dev/google/elmo/3\", trainable=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPEwBk_0NfbP"
      },
      "source": [
        "# Data Prerequisites\n",
        "!mkdir -p texts\n",
        "\n",
        "from_raw_texts = False # Stanza takes a while to download, loading from raw texts is slower than just opening a pre-tokenized dataframe\n",
        "if from_raw_texts:\n",
        "\n",
        "  # Get text files\n",
        "  !wget -q https://www.dropbox.com/s/5ibk0k4mibcq3q6/AussieTop100private.zip?dl=1 -O ./texts/AussieTop100private.zip\n",
        "  !unzip -qq -n -d ./texts/ ./texts/AussieTop100private.zip\n",
        "\n",
        "  about_dir = Path.cwd() / \"texts\" / \"About\"\n",
        "  pr_dir = Path.cwd() / \"texts\" / \"PR\"\n",
        "  dirs_to_load = [about_dir, pr_dir]\n",
        "\n",
        "\n",
        "  # Preprocess texts\n",
        "  !pip -q install stanza\n",
        "  import stanza\n",
        "  stanza.download('en')\n",
        "  nlp = stanza.Pipeline('en', processors='tokenize,pos', tokenize_no_ssplit=True)\n",
        "\n",
        "  texts = [] \n",
        "  nltk.download(\"stopwords\", quiet=True)\n",
        "  stops = nltk.corpus.stopwords.words('english')+[\"'s\", '&']\n",
        "  for directory in dirs_to_load:\n",
        "    for file in glob.glob(f\"{directory}/*.txt\"):\n",
        "      with open(file, 'r') as infile: \n",
        "        fulltext = \" \".join([word.text.lower() for sentence in nlp(infile.read()).sentences for word in sentence.words if word.text.lower() not in stops and word.upos not in [\"PUNCT\", \"SYM\", \"NUM\", 'X']])\n",
        "        texts.append(fulltext)\n",
        "  text_df = pd.DataFrame(texts, columns=['text_no_stops'])\n",
        "  \n",
        "else:\n",
        "  # Get pretokenized text dataframe\n",
        "  !wget -q https://www.dropbox.com/s/o6b8hxeq8zpvxgj/pretokenized_aussie_fbs.csv?dl=1 -O ./texts/pretokenized_aussie_fbs.csv\n",
        "  text_df = pd.read_csv('./texts/pretokenized_aussie_fbs.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sh9IDjDl5yXv"
      },
      "source": [
        "#Module 7 - Word Representations\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFeIDYUm54Dw"
      },
      "source": [
        "At a fundamental level, computers know two things: on(1) and off(0). Building from this we're able to teach computers other numbers pretty easily by stringing these ons/offs together (e.g., 101101 in binary = 45 in decimal). However, the notion of 'text' or even 'words' is not natively understood by computers. As a result, for computers to understand text, we have to convert that text into numbers. But how to do that?\n",
        "\n",
        "In this module, we'll look at four major models of the way that we present words to the computer for analysis:\n",
        "* Bag-of-Words model\n",
        "* GloVe model\n",
        "* Contextual embedding models (ELMo, BERT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DXIiPfz-xRI"
      },
      "source": [
        "#7.1. Bag-of-Words Model\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3A7KFgTjxfp"
      },
      "source": [
        "In the Bag-of-Words model, each word is represented as a one-hot vector in a vector space with the same dimensionality as the number of unique words in the corpus (i.e., the vocabulary). \n",
        "\n",
        "Consider the three words: 'leaders', 'managers', and 'entrepreneurs':"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkLWFGzhk1ik"
      },
      "source": [
        "# Displays the one-hot vector representation of three words\n",
        "words = [\"managers\", \"leaders\", \"entrepreneurs\"]\n",
        "vectorizer = CountVectorizer()\n",
        "doc_term_matrix = vectorizer.fit_transform(words)\n",
        "print(f\"There are {len(vectorizer.get_feature_names())} words in the vocabulary: {vectorizer.get_feature_names()}\\n\")\n",
        "for text_id, text in enumerate(words):\n",
        "  print(f\"Word {text_id+1}: {words[text_id]:15} --- As a one-hot vector: {doc_term_matrix.toarray()[text_id]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h055CPtEk117"
      },
      "source": [
        "If words are one-hot vectors, entire documents can then be expressed as combinations of these vectors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycZI4InZ-l4j"
      },
      "source": [
        "# Shows how one-hot vectors accumulate into word frequencies\n",
        "sample_texts = [\"Entrepreneurs and managers, while similar, also face a number of idiosyncratic challenges.\", \n",
        "                \"Managers and entrepreneurs are similar. While similar, managers and entrepreneurs face idiosyncratic challenges.\"]\n",
        "vectorizer = CountVectorizer()\n",
        "doc_term_matrix = vectorizer.fit_transform(sample_texts)\n",
        "print(f\"There are {len(vectorizer.get_feature_names())} words in the vocabulary: {vectorizer.get_feature_names()}\")\n",
        "for text_id, text in enumerate(sample_texts):\n",
        "  print(f\"\\nText {text_id+1}: {sample_texts[text_id]}\\n As a Bag of Words: {doc_term_matrix.toarray()[text_id]}\\n\")\n",
        "  for word_id, word in enumerate(doc_term_matrix.toarray()[text_id]):\n",
        "    print(f\"Word: {vectorizer.get_feature_names()[word_id]:15} - {word}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ut_Z9cRNeNp1"
      },
      "source": [
        "This representation is very convenient for basic analyses based on word counts and it enables us to look at how similar two texts are to eachother by comparing the similarity of two documents' arrays:\n",
        "\n",
        "*Note: we'd actually do a bit more preprocessing before comparing the similarity of these two documents, but we'll put that aside for now.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X09r7ze5em9F"
      },
      "source": [
        "# Displays the cosine similarity between the two sample texts\n",
        "print(f\"The cosine similarity of the two texts is: {cosine_similarity(doc_term_matrix)[0][1]:0.2}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAZPkQhSKP-x"
      },
      "source": [
        "However, as we start working with larger texts this representation of text leads to very sparse (filled with lots of zeroes) matrices. Consider the sample of Australian family business press releases and about us pages that were loaded by the *prerequisites* code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "068xZJfcJlB7"
      },
      "source": [
        "# Shows an excerpt of a real (i.e., sparse) document-term matrix \n",
        "vectorizer = CountVectorizer()\n",
        "doc_term_matrix = vectorizer.fit_transform(text_df['text_no_stops'])\n",
        "print(f\"There are {len(vectorizer.get_feature_names())} words in the vocabulary: {vectorizer.get_feature_names()}\\n\")\n",
        "print(f\"Text 1: {text_df.iloc[0]['text_no_stops']}\\n As a Bag of Words: \\n{doc_term_matrix.toarray()[0][0:1000]}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywwhppF2c3uL"
      },
      "source": [
        "And that's just the first 1,000 words (of nearly 7,900) for just the first text in our corpus. Storing and processing that many zeroes is pretty inefficient. \n",
        "\n",
        "Further, intuitively, we know that two words (e.g., 'manager' and 'leader') have similar meaning. However, this representation of words treats all individual words as being orthogonal. So while we can compare entire documents, the words that comprise those documents generally cannot be compared. Consider:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfM3BQiLg4BN"
      },
      "source": [
        "# Calculates and shows the orthogonality of words in the bag-of-words representation\n",
        "words = [\"managers\", \"leaders\", \"entrepreneurs\"]\n",
        "vectorizer = CountVectorizer()\n",
        "doc_term_matrix = vectorizer.fit_transform(words)\n",
        "print(f\"There are {len(vectorizer.get_feature_names())} words in the vocabulary: {vectorizer.get_feature_names()}\\n\")\n",
        "for text_id, text in enumerate(words):\n",
        "  print(f\"Word {text_id+1}: {words[text_id]:15} --- As a one-hot vector: {doc_term_matrix.toarray()[text_id]}\")\n",
        "\n",
        "print(f\"\\nThe cosine similarity of 'managers' and 'leaders' is: {cosine_similarity(doc_term_matrix)[0][1]:0.2}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCUKIVTFhteF"
      },
      "source": [
        "Combine all this with the notion that the Bag of Words model ignores word order and the limitations of this representation become fairly apparent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQzNrEyb-3ha"
      },
      "source": [
        "#7.2. GloVe Word Embeddings\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qhKWi2Dh_5b"
      },
      "source": [
        "Rather than encoding information about word frequency into sparse vectors, GloVe creates *neural word embeddings* that are dense (no zeroes) and lower-dimensional (e.g., 50-300 vs 7900) vectors that and encode information about how that word is used in natural language. \n",
        "\n",
        "The GloVe vectors themselves can be created using a *unsupervised machine learning* algorithm on your corpus of texts; however, in this case, we're going to use the vectors provided by Stanford based on language use in both Wikipedia and news articles (i.e., Gigaword 5).\n",
        "\n",
        "Consider the GloVe vector for the word 'managers':"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4SZsvqk--y0"
      },
      "source": [
        "# Displays the GloVe embedding for the word 'managers'\n",
        "word = 'managers'\n",
        "print(f\"GloVe embedding for '{word}' has {glove_model.get_vector(word).shape[0]} dimensions\")\n",
        "print(f\"GloVe embedding for '{word}': \\n{glove_model.get_vector(word)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LetQgNJGq7_8"
      },
      "source": [
        "Because we have information about how the word is used encoded into the vector, we can now provide that direct comparison of words that we could not do with the Bag of Words model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhxJfoccva6T"
      },
      "source": [
        "# Calculates the cosine similarity of two words with GloVe embeddings\n",
        "comparison_word = 'leaders'\n",
        "print(f\"The cosine similarity of '{word}' and '{comparison_word}' is: {cosine(glove_model.get_vector(word), glove_model.get_vector(comparison_word))}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3WMXPd1xh1e"
      },
      "source": [
        "We can also work the other direction to find the words that are most similar to a specified word in the Wikipedia/News corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLe9aUJkxtKL"
      },
      "source": [
        "# Displays the ten most similar words to 'managers' in GloVe embeddings\n",
        "glove_model.most_similar(word, topn=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecGdUNr8yDtK"
      },
      "source": [
        "# Displays the ten most similar words to 'entrepreneurs' in GloVe embeddings\n",
        "comparison_word2 = 'entrepreneurs'\n",
        "glove_model.most_similar(comparison_word2, topn=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oUeFCLFyNeQ"
      },
      "source": [
        "Because the usage of each vector is encoded into 300-dimension space, we can use matrix algebra to look for connections between words as well. For example, we'll take a look at a classic example:\n",
        "\n",
        "Let's start with the vector for 'king', subtract the vector for 'man', and add the vector for 'woman':\n",
        "\n",
        "`'king' - 'man' + 'woman'`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kHwwtwTDLmG"
      },
      "source": [
        "# Demonstrates simple vector algebra with GloVe embeddings\n",
        "glove_model.most_similar(positive=['king', 'woman'], negative=['man'], topn=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIbUkhp3zosA"
      },
      "source": [
        "That makes sense, how about to more abstract concepts?\n",
        "\n",
        "`'clinton' - 'democrat' + 'republican'`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz3gDNrkzz26"
      },
      "source": [
        "# Demonstrates more abstract vector algebra with GloVe embeddings\n",
        "glove_model.most_similar(positive=['clinton', 'republican'], negative=['democrat'], topn=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qr8KceKWzG3E"
      },
      "source": [
        "That makes sense, but we're business scholars...let's apply this to a business setting: \n",
        "\n",
        "`'businessman' - 'man' + 'woman'`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyDbIKeozYxu"
      },
      "source": [
        "glove_model.most_similar(positive=['businessman', 'woman'], negative=['man'], topn=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-edwt0hGzG4d"
      },
      "source": [
        "`'businessman' + 'innovation'`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUEfGBTZ0R61"
      },
      "source": [
        "glove_model.most_similar(positive=['businessman', 'innovation'], topn=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEiFn-Mo06Hv"
      },
      "source": [
        "Let's visualize what is happening here by projecting the 300-dimensional space into 3-dimensional space using Principal Components Analysis:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKGMvkZJ2JS9"
      },
      "source": [
        "# Plots PCA of the words in the word list into 3-dimensional space\n",
        "word_list = ['businessman', 'entrepreneur', 'leader', 'manager', 'businesswoman', 'employee', 'employees', 'managers',\n",
        "             'entrepreneurs',  'executive', 'executives', 'industrialist', 'industrialists', 'leaders', 'businesswomen', \n",
        "             'businessmen', 'businessperson', 'businesspeople', 'school', 'schools', 'university', 'universities', 'professor', \n",
        "             'professors', 'instructor', 'instructors', 'teacher', 'teachers', 'institute', 'institutes', 'college', 'colleges']\n",
        "glove_vectors = [glove_model[word] for word in word_list]\n",
        "project3d = PCA().fit_transform(glove_vectors)[:,:3]\n",
        "word_projection_map = []\n",
        "for word, (x,y,z) in zip(word_list, project3d):\n",
        "  word_projection_map.append({'word': word, 'x': x, 'y': y, 'z':z})\n",
        "embedding_df = pd.DataFrame(word_projection_map)\n",
        "\n",
        "fig = px.scatter_3d(embedding_df, x=\"x\", y=\"y\", z=\"z\", text=\"word\")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5coWywQc7nU4"
      },
      "source": [
        "#7.3. Contextual Word Embeddings\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6ZJKDmj8AZR"
      },
      "source": [
        "I'm lumping a pretty diverse of embedding models together here; however, for our purposes in this module, we're going to treat them as the same despite important differences.\n",
        "\n",
        "In the previous section we looked at GloVe and Word2Vec which learn embeddings based on the context in which words are used in a corpus. This is fantastic, but once these embeddings are learned, each instance of that 'word' is treated the same. Intuition tells us that shouldn't be the case... consider \"work\":\n",
        "\n",
        "* The employee went to work every morning,\n",
        "* I work very hard for my pay,\n",
        "* This is a masterful work of art,\n",
        "* I was tired after a long day's work, and\n",
        "* My cellphone doesn't work.\n",
        "\n",
        "Each sentence contains the word 'work', but while it's spelled the same, the meaning is very different. Contextual word embeddings (e.g., ELMo, BERT) do not store a single embedding for each word - the embedding of the word is contextualized both during training and when it's being applied.\n",
        "\n",
        "Consider the example of 'work' in these sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkdtHsj0c-o6"
      },
      "source": [
        "# Uses ELMo embeddings to show how 'work' has different embeddings depending on how it is used.\n",
        "texts = [\"The employee went to work every morning\", \n",
        "         \"I work very hard for my pay\", \n",
        "         \"This is a masterful work of art\", \n",
        "         \"I was tired after a long day's work\", \n",
        "         \"My cellphone doesn't work\"]\n",
        "work_locations = [(text_id, text.split(\" \").index(\"work\")) for text_id, text in enumerate(texts)]\n",
        "\n",
        "work_embeddings = elmo(texts, signature=\"default\", as_dict=True)[\"elmo\"]\n",
        "with tf.Session() as session:\n",
        "  session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "  elmo_embeddings = session.run(work_embeddings)\n",
        "\n",
        "print(\"\\nHere are the first 5 elements of the embeddings for the five 'work's:\")\n",
        "for text_id, word_id in work_locations:\n",
        "  print(f\"Sentence {text_id+1}: {elmo_embeddings[text_id][word_id][:5]}\")\n",
        "\n",
        "for (text1, word1), (text2, word2) in combinations(work_locations, 2):\n",
        "  print(f\"\\nSimilarity of 'work' in the sentences: '{texts[text1]}' and '{texts[text2]}': \")\n",
        "  print(cosine(elmo_embeddings[text1][word1], elmo_embeddings[text2][word2]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRP7jX4Akwr1"
      },
      "source": [
        "We can see that even though each sentence uses the word 'work', the embedding of the word changes based on the context in which 'work' is being used."
      ]
    }
  ]
}