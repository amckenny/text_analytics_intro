{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "06_text_visualization.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNQutPAUrpCJlEpFpqe+YDZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amckenny/text_analytics_intro/blob/main/notebooks/06_text_visualization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epfc8xC4i1j-"
      },
      "source": [
        "#Prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFxlcVe9hZeC"
      },
      "source": [
        "# Get external files and install 3rd party packages\n",
        "!wget -q https://www.dropbox.com/s/vbcpntdzt1weid2/australia.png?dl=1 -O ./australia.png\n",
        "!mkdir -p texts\n",
        "!wget -q https://www.dropbox.com/s/5ibk0k4mibcq3q6/AussieTop100private.zip?dl=1 -O ./texts/AussieTop100private.zip\n",
        "!unzip -qq -n -d ./texts/ ./texts/AussieTop100private.zip\n",
        "!pip -q install scattertext\n",
        "!pip -q install stanza\n",
        "\n",
        "# Standard library imports\n",
        "import glob, re\n",
        "from pathlib import Path\n",
        "from IPython.core.display import display, HTML\n",
        "from collections import Counter\n",
        "\n",
        "# 3rd party imports\n",
        "import nltk, stanza\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scattertext as st\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, ImageColorGenerator\n",
        "\n",
        "sns.set_theme()\n",
        "\n",
        "# Spin up stanza pipeline for tokenization\n",
        "nltk.download(\"stopwords\", quiet=True)\n",
        "stanza.download('en')\n",
        "nlp = stanza.Pipeline('en', processors='tokenize,mwt,pos', tokenize_no_ssplit=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvrmvT89qFoO"
      },
      "source": [
        "#Preprocess texts\n",
        "about_dir = Path.cwd() / \"texts\" / \"About\"\n",
        "pr_dir = Path.cwd() / \"texts\" / \"PR\"\n",
        "dirs_to_load = [about_dir, pr_dir]\n",
        "\n",
        "texts = [] \n",
        "stops = nltk.corpus.stopwords.words('english')+[\"'s\", '&']\n",
        "for directory in dirs_to_load:\n",
        "  for file in glob.glob(f\"{directory}/*.txt\"):\n",
        "    with open(file, 'r') as infile: \n",
        "      text_type = file.split(\"/\")[-2]\n",
        "      text_id = file.split(\"/\")[-1]\n",
        "      fulltext = infile.read()\n",
        "      tokens = [word.text.lower() for sentence in nlp(fulltext).sentences for word in sentence.words if word.upos not in [\"PUNCT\", \"SYM\", \"NUM\", 'X']]\n",
        "      texts.append({'text_type': text_type, 'text_id': text_id, 'text': fulltext, 'tokens': tokens})\n",
        "text_df = pd.DataFrame(texts)\n",
        "text_df['token_no_stops'] = text_df['tokens'].apply(lambda x: [word for word in x if word not in stops])\n",
        "text_df['text_no_stops'] = text_df['token_no_stops'].apply(lambda x: \" \".join([word for word in x]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DV4aH8dGnOjl"
      },
      "source": [
        "#Australia word cloud mask initialization\n",
        "aus_img = Image.open(\"australia.png\")\n",
        "mask = np.array(aus_img)\n",
        "\n",
        "def transform_mask(pixel):\n",
        "  if pixel == 0:\n",
        "    return 255\n",
        "  else:\n",
        "    return pixel\n",
        "\n",
        "tf_mask = np.ndarray((mask.shape[0],mask.shape[1]), np.int32)\n",
        "for i in range(len(mask)):\n",
        "  tf_mask[i] = list(map(transform_mask, mask[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xM3NJSV8i56D"
      },
      "source": [
        "#Module 6 - Text Visualization\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2260VAc6QPJ6"
      },
      "source": [
        "As researchers, we're often told to get to know our data before analyzing it. The same holds true in text analysis as well. We'd like to know 'what is in' our corpus of texts at a high level before conducting analyses.\n",
        "\n",
        "In this module, we'll look at a number of different ways of visualizing our data - both graphically and statistically. The goals for this module are:\n",
        "\n",
        "* View document length distributions\n",
        "* Generate word frequency distributions and see Zipf's law in action\n",
        "* Find n-grams and generate PMI statistics\n",
        "* Use concordance analysis to see the context of tokens/phrases\n",
        "* Create a word cloud"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45Ycs1n_P3o8"
      },
      "source": [
        "##6.1. Basic Visualizations\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPuAhZOpPbKa"
      },
      "source": [
        "###6.1.1. Document Length Distribution\n",
        "One of the first things we often look at when 'getting to know our texts' is the length of the texts themselves. We usually find a positively skewed distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgaMygIV4GIQ"
      },
      "source": [
        "# Displays a histogram of corpus word counts\n",
        "text_df['word_count'] = text_df['tokens'].apply(lambda x: len(str(x)))\n",
        "text_df.hist(bins=20, grid=True, figsize=(20,5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfKU_wheTLsA"
      },
      "source": [
        "However, we've combined two different types of documents here... let's look at the distribution by document type."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-4Ck28ERaD4"
      },
      "source": [
        "# Displays a histogram of corpus word counts for each type of text\n",
        "text_df.hist(bins=20, by=\"text_type\", grid=True, figsize=(20,5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlO8HGDHUHKH"
      },
      "source": [
        "###6.1.2. Word Frequency Distributions\n",
        "\n",
        "We may also want to understand what is being said in these texts in general before we start doing deeper analyses. Let's look at the most frequently used words in the corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXNolAuLh1WY"
      },
      "source": [
        "# Creates a frequency distribution plot for the words in the corpus\n",
        "freq_dist = nltk.FreqDist(word for id, row in text_df.iterrows() for word in row['tokens'])\n",
        "plt.figure(figsize=(20,5))\n",
        "freq_dist.plot(50, title=\"Frequency distribution of the top 50 words\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExLY4KBTVUNT"
      },
      "source": [
        "This illustrates a couple of things:\n",
        "1. The value of stopword removal: 'the', 'and', 'of', 'to', etc. are frequent, but they tell us little regarding the contents of the text\n",
        "2. Zipf's law appears to hold even in our small corpus of texts.\n",
        "\n",
        "Let's remove stopwords and look at the distribution again:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSiiTCwcDPC_"
      },
      "source": [
        "# Creates a frequency distribution plot for the words in the corpus (minus stopwords)\n",
        "freq_dist = nltk.FreqDist(word for id, row in text_df.iterrows() for word in row['token_no_stops'])\n",
        "plt.figure(figsize=(20,5))\n",
        "freq_dist.plot(50, title=\"Frequency distribution of the top 50 words\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kOuU9WsWTrv"
      },
      "source": [
        "This distribution is reflective of Zipf's law, but the frequencies don't seem to drop off as quickly as the law might specify. I would suggest that in a larger corpus, it would.\n",
        "\n",
        "We also see that the 'content' of the texts seem a lot more apparent. These texts seem to talk about australian businesses, with an emphasis on things that are new about these businesses. This is pretty accurate given the corpus of texts.\n",
        "\n",
        "However, we're still looking at the overall frequency. Let's look at a conditional frequency distribution and see if any patterns emerge."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLsI3VTZYKkK"
      },
      "source": [
        "# Creates a conditional frequency distribution conditioned on the type of text\n",
        "cfd = nltk.ConditionalFreqDist((row['text_type'], word) for id, row in text_df.iterrows() for word in row['token_no_stops'])\n",
        "plt.figure(figsize=(20,5))\n",
        "cfd.plot(conditions=[\"About\", \"PR\"], samples=list(list(zip(*freq_dist.most_common(50))))[0], title=\"Conditional frequency distribution of the top 50 words\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbVPvUggkQL2"
      },
      "source": [
        "This is interesting; however, there are significantly more words in our corpus for press releases than for about us webpages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3yuvfvslsui"
      },
      "source": [
        "# Calculates and displays the total number of words for each type of texts\n",
        "about_words = text_df.groupby('text_type')['word_count'].sum()['About']\n",
        "pr_words = text_df.groupby('text_type')['word_count'].sum()['PR']\n",
        "print(f\"There are a total of {pr_words} in the press release texts, whereas there are only {about_words} in the About Us pages.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bYXU30tnbXp"
      },
      "source": [
        "A better apples-to-apples comparison would look at the conditional **relative** frequency distribution of the words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Baphi0_xnly8"
      },
      "source": [
        "# Displays the conditional frequency distribution as percentages rather than raw word counts\n",
        "for word in freq_dist:\n",
        " cfd['About'][word] /= about_words\n",
        " cfd['PR'][word] /= pr_words\n",
        "\n",
        "plt.figure(figsize=(20,5))\n",
        "cfd.plot(conditions=[\"About\", \"PR\"], samples=list(list(zip(*freq_dist.most_common(50))))[0], title=\"Conditional relative frequency distribution of the top 50 words\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbLjrpQPseWs"
      },
      "source": [
        "###6.1.3. Distinctive Words\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YBftpWtszww"
      },
      "source": [
        "The conditional frequency distributions above show us that among the most frequent words used in the corpus there are several words that are more frequently used in one type of text than in another.\n",
        "\n",
        "However, using `scattertext` we can view such distinctive words in a clearer and more interactive way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8Bglf-Stccs"
      },
      "source": [
        "# Creates a scattertext diagram for the corpus with the percential_dense transformation\n",
        "st_corpus = st.CorpusFromPandas(text_df, category_col='text_type', text_col='text_no_stops').build().compact(st.AssociationCompactor(2000))\n",
        "st_html = st.produce_scattertext_explorer(st_corpus,\n",
        "                                          category='About',\n",
        "                                          category_name='About Us',\n",
        "                                          not_categories=['PR'],\n",
        "                                          sort_by_dist=False,\n",
        "                                          term_scorer=st.CredTFIDF(st_corpus),\n",
        "                                          metadata=text_df['text_id'],\n",
        "                                          transform=st.Scalers.percentile_dense)\n",
        "HTML(st_html)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqOZk3d1y3xR"
      },
      "source": [
        "This view is similar to what we saw before in that we see a lot more variance around the most frequently used words. However, here we have much more context. \n",
        "\n",
        "We can change the scaling to see more variance in the less-frequent words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TS3BWROeyfwb"
      },
      "source": [
        "# Creates a scattertext diagram for the corpus without the percential_dense transformation\n",
        "st_html = st.produce_scattertext_explorer(st_corpus,\n",
        "                                          category='About',\n",
        "                                          not_categories=['PR'],\n",
        "                                          sort_by_dist=False,\n",
        "                                          term_scorer=st.CredTFIDF(st_corpus),\n",
        "                                          metadata=text_df['text_id'])\n",
        "HTML(st_html)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8DX6EyW5RTF"
      },
      "source": [
        "##6.2. N-grams\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmKM-0V3A-D8"
      },
      "source": [
        "Whereas previously we have largely focused on understanding the documents one word at a time, we may also be interested in groupings of words that appear frequently in the corpus (appropriately called n-grams).\n",
        "\n",
        "Let's take a look at the most frequent word sequences (i.e., bigrams, trigrams):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gntGLdkRfXsY"
      },
      "source": [
        "# Calculates and displays a frequency distribution for the most frequent bigrams\n",
        "freq_dist = nltk.FreqDist(\" \".join(bigram) for id, row in text_df.iterrows() for bigram in nltk.ngrams(row['tokens'], 2))\n",
        "plt.figure(figsize=(20,5))\n",
        "freq_dist.plot(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrSCLm-LFanp"
      },
      "source": [
        "# Calculates and displays a frequency distribution for the most frequent trigrams\n",
        "freq_dist = nltk.FreqDist(\" \".join(bigram) for id, row in text_df.iterrows() for bigram in nltk.ngrams(row['tokens'], 3))\n",
        "plt.figure(figsize=(20,5))\n",
        "freq_dist.plot(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEsEDrypNQuX"
      },
      "source": [
        "Clearly the most common n-grams frequently contain stop words (e.g., \"of the,\" \"as well as\"). Sometimes this is desirable to capture phrases like \"products and services\" that reflect meaningful phrases that include a stop word. However, it's also often valuable to complement this with an examination with stop words removed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb5SDdgyQLOV"
      },
      "source": [
        "# Calculates and displays a frequency distribution for the most frequent bigrams (with stopwords removed)\n",
        "freq_dist = nltk.FreqDist(\" \".join(bigram) for id, row in text_df.iterrows() for bigram in nltk.ngrams(row['token_no_stops'], 2))\n",
        "plt.figure(figsize=(20,5))\n",
        "freq_dist.plot(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpzMgN_9QPoc"
      },
      "source": [
        "# Calculates and displays a frequency distribution for the most frequent trigrams (with stopwords removed)\n",
        "freq_dist = nltk.FreqDist(\" \".join(bigram) for id, row in text_df.iterrows() for bigram in nltk.ngrams(row['token_no_stops'], 3))\n",
        "plt.figure(figsize=(20,5))\n",
        "freq_dist.plot(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4Njyh5rML4e"
      },
      "source": [
        "Looking at the pointwise mutual information can provide us with complementary insight by showing us words that show up most frequently as part of an n-gram (versus in other contexts)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhd-LPAzKqaa"
      },
      "source": [
        "# Identifies and displays the bigrams with the highest PMI scores (and which appear at least ten times)\n",
        "bigrams = nltk.collocations.BigramCollocationFinder.from_documents(text_df['tokens'])\n",
        "bigrams.apply_freq_filter(10)\n",
        "for idx, (bigram, pmi) in enumerate(bigrams.score_ngrams(nltk.collocations.BigramAssocMeasures.pmi)):\n",
        "  print(f\"Bigram = {' '.join(bigram):30}--- Frequency = {bigrams.ngram_fd[bigram]} --- PMI = {pmi} \")\n",
        "  if idx > 19:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50b2V9ZmOxpZ"
      },
      "source": [
        "# Identifies and displays the trigrams with the highest PMI scores (and which appear at least ten times)\n",
        "trigrams = nltk.collocations.TrigramCollocationFinder.from_documents(text_df['tokens'])\n",
        "trigrams.apply_freq_filter(10)\n",
        "for idx, (trigram, pmi) in enumerate(trigrams.score_ngrams(nltk.collocations.TrigramAssocMeasures.pmi)):\n",
        "  print(f\"Trigram = {' '.join(trigram):30}--- Frequency = {trigrams.ngram_fd[trigram]} --- PMI = {pmi} \")\n",
        "  if idx > 19:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZrhnZgWXHHP"
      },
      "source": [
        "##6.3. Concordance/KWIC Analysis\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGg2mNj8XQYI"
      },
      "source": [
        "Having looked at what individual words and phrases appear to be important in the corpus, it's often valuable to look at a concordance/keyword in context (KWIC) analysis.\n",
        "\n",
        "NLTK offers this natively for individual words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqHhD-IPXG1e"
      },
      "source": [
        "# Presents a concordance/KWIC analysis for the word 'australia' in the corpus\n",
        "look_for = \"australia\"\n",
        "nltk.Text([word for id, row in text_df.iterrows() for word in row['tokens']]).concordance(look_for)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulRU3892dvIF"
      },
      "source": [
        "The nltk version cannot handle phrases. However, it is not difficult to create our own version that can:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYsFs14lehLL"
      },
      "source": [
        "# Creates a concordance analysis function that can handle phrases in addition to individual words\n",
        "def phrase_concordance(text, phrase, window=40, lines=25):\n",
        "  locations = [idx.start() for idx in re.finditer(phrase, text)]\n",
        "  if lines > len(locations):\n",
        "    print(f\"Displaying all {len(locations)} matches:\")\n",
        "    for location in locations:\n",
        "      print(text[location-window:location+len(phrase)+window])\n",
        "  else:\n",
        "    print(f\"Displaying {lines} of {len(locations)} matches:\")\n",
        "    for idx, location in enumerate(locations):\n",
        "      print(text[location-window:location+len(phrase)+window])\n",
        "      if idx == lines-1:\n",
        "        break\n",
        "\n",
        "# Presents a concordance/KWIC analysis for the phrase 'chief executive officer' in the corpus\n",
        "phrase = \"chief executive officer\"\n",
        "text = \" \".join([\" \".join(row['tokens']) for id, row in text_df.iterrows()])\n",
        "phrase_concordance(text, phrase)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3oDPVQBkau5"
      },
      "source": [
        "#6.4. Word Clouds\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mlyj3FHHsFz6"
      },
      "source": [
        "A valuable way of conveying a lot of information about a corpus of texts (or even a subset thereof) is to present readers with a wordcloud. There are a lot of ways to present word clouds. However, the most basic form presents the word cloud with size being proportional to frequency:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzVFLcFMkZ01"
      },
      "source": [
        "# Creates a word cloud for the corpus\n",
        "wordcloud = WordCloud(background_color=\"white\", max_words = 150, stopwords=stops).generate(text)\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjumkDf0sPXT"
      },
      "source": [
        "You can also prepare a 'mask' image for the word cloud to fit. In this way, the generated word cloud will fit inside of the mask to represent something relevant to the corpus - in this case, the political boundaries of Australia:\n",
        "\n",
        "*Note: If you're looking for it, the mask preprocessing was done in the 'prerequisites' section*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hhcJbuzrGk9"
      },
      "source": [
        "# Creates a word cloud for the corpus using an Australia mask\n",
        "wordcloud = WordCloud(background_color=\"white\", max_words = 150, stopwords=stops, mask=tf_mask).generate(text)\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}