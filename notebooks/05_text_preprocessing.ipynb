{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05_text_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNdGBSujbVdIThxj+CNh/K3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amckenny/text_analytics_intro/blob/main/notebooks/05_text_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ote3p0WvqK8X"
      },
      "source": [
        "#Prerequisites\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0bzUkFMpase"
      },
      "source": [
        "# Get external files\n",
        "!mkdir -p texts\n",
        "!wget -q https://www.dropbox.com/s/5ibk0k4mibcq3q6/AussieTop100private.zip?dl=1 -O ./texts/AussieTop100private.zip\n",
        "!unzip -qq -n -d ./texts/ ./texts/AussieTop100private.zip\n",
        "\n",
        "# Standard library imports\n",
        "import glob, string\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "# 3rd party imports\n",
        "import nltk, nltk.sentiment\n",
        "from nltk.corpus import wordnet as wn\n",
        "import pandas as pd\n",
        "\n",
        "# Downloads nltk corpora for preprocessing tasks\n",
        "nltk.download(\"stopwords\", quiet=True)\n",
        "nltk.download(\"punkt\", quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "\n",
        "# Creates path variables to texts\n",
        "about_dir = Path.cwd() / \"texts\" / \"About\"\n",
        "pr_dir = Path.cwd() / \"texts\" / \"PR\"\n",
        "dirs_to_load = [about_dir, pr_dir]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhmcys5Sq4ye"
      },
      "source": [
        "#Module 5 - Text Preprocessing\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um41lpqNrL_v"
      },
      "source": [
        "There are lots of decisions to make with regards to text preprocessing. Which ones are most relevant depends on a lot of factors - more than can reasonably discussed here. I don't agree with *all* recommendations they make, however [this article](https://journals.sagepub.com/doi/pdf/10.1177/1094428120971683) does a good job in outlining key techniques and considerations for text preprocessing.\n",
        "\n",
        "In this module, we'll introduce a cornucopia of text preprocessing options and demonstrate how they can be done in Python. The goals for this module are:\n",
        "\n",
        "* Demonstrate segmentation\n",
        "* Demonstrate tokenization\n",
        "* Demonstrate case conversion\n",
        "* Demonstrate non-word character removal\n",
        "* Demonstrate token replacement\n",
        "* Demonstrate stop word removal\n",
        "* Demonstrate stemming/lemmatization\n",
        "\n",
        "**Notes**: These are not presented in a prescribed order. They are ordered for pedagogical convenience."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pw9qdQRG288p"
      },
      "source": [
        "##5.1. Segmentation\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SftIupfhHwA5"
      },
      "source": [
        "The *prerequisites* code for this module downloads a corpus of 'about us' webpages and press releases from Australian family businesses. However, sometimes our level of analysis may be lower than the 'text' level. In these cases we use 'segmentation' to transform the full text into a list of segments.\n",
        "\n",
        "For this example we will use sentence segmentation. Handily, the NLTK package (Python's jack-of-all-trades-and-master-of-none for natural language processing) has a sentence segmenter built in.\n",
        "\n",
        "Let's see what it can do:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUXiFsYqIxqs"
      },
      "source": [
        "# Segments a string of sentences into a list of sentences and displays the list.\n",
        "sentences = \"Mr. Davis and Mrs. Garvey were handed the treatment options for their son by Dr. Smith. \"\\\n",
        "            \"They were pleased to see that it would not be too expensive! \"\\\n",
        "            \"However, they were still hopeful that there would be outpatient options available.\"\n",
        "nltk.tokenize.sent_tokenize(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBvZq5D8MbgW"
      },
      "source": [
        "Perfect. Note how the segmenter was not fooled by the periods after Mr, Dr, and Mrs... and it knows that exclamation marks also delimit sentences. This is much easier than us doing it manually with a `split()` method as we did previously."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPkcD_uljnqR"
      },
      "source": [
        "##5.2. Tokenization\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoqbdNe2j4L0"
      },
      "source": [
        "Tokens in text analysis refer to the discrete units of meaning we want to analyze. In our case, that means individual words. Accordingly, tokenization is going to turn our text into a list of individual words. Let's take a look:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tZlALbMm5V7"
      },
      "source": [
        "# Breaks a string into a list of tokens and displays the list.\n",
        "sentences = \"This is a test. This will break it into words.\"\n",
        "word_tokenize = nltk.tokenize.word_tokenize\n",
        "word_tokenize(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "go8yvn6bsJWq"
      },
      "source": [
        "Note that the word tokenizer broke the text into words, but not into sentences. If your unit of analysis is the sentence, you'll generally want to segment before tokenizing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lw8DQc3VtZhm"
      },
      "source": [
        "##5.3. Case Conversion\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZxJvyButfzk"
      },
      "source": [
        "As we saw in a previous module, Python treats words with different casing as being different, even if they are the same word:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEFmTTvhtoIp"
      },
      "source": [
        "# Demonstrates the inequality of differently cased strings\n",
        "\"This\" == \"this\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7NqjGe1tqXn"
      },
      "source": [
        "For many text analyses, we do not care about the casing of the words. As a result it is common to make everything lowercase to facilitate comparisons/grouping.\n",
        "\n",
        "Consider the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJdBcvpot-Eg"
      },
      "source": [
        "# Creates and displays Counter objects for lists of words with and without case conversion\n",
        "list_of_words = [\"Innovative\", \"innovative\", \"INNOVATIVE\"]\n",
        "word_counter = Counter(list_of_words)\n",
        "print(f\"Before case conversion, the word counts are: {word_counter}\")\n",
        "\n",
        "list_of_words = [word.lower() for word in list_of_words]\n",
        "word_counter = Counter(list_of_words)\n",
        "print(f\"After case conversion, the word counts are: {word_counter}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ5WeF-BxzTB"
      },
      "source": [
        "You can see that converting the words to lowercase enables us to count the number of times that the word 'innovative' appears in the text independent of its case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AVG0_JIw2_Z"
      },
      "source": [
        "##5.4. Non-Word Character Removal\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkyQpE2gxGRA"
      },
      "source": [
        "Just as we're often not concerned with the casing of words, we're often not concerned with non-word characters. For instance, numerals, punctuation, typesetting, empty spaces, and so on. Depending on the task at hand, you may want to remove or replace these characters.\n",
        "\n",
        "There are a number of different ways to do this, here's one way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1cJ9UcbzVDu"
      },
      "source": [
        "# Displays the tokenized sentence with and without non-word character removal\n",
        "list_of_words = [\"this\", \"sentence\", \"has\", \"numerals\", \",\", \"such\", \"as\", \"0123\", \",\", \"and\", \"punctuation\", \"!\"]\n",
        "print(f\"Before non-word character removal: {list_of_words}\")\n",
        "\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "list_after_nwc_removal = [word.translate(table) for word in list_of_words if word.translate(table) and not word.isdigit()]\n",
        "print(f\"After non-word character removal: {list_after_nwc_removal}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZ_gfv_61JzX"
      },
      "source": [
        "Sometimes flawed optical character recognition in PDFs or encoding errors may result in non-word characters beyond punctuation, numerals, etc. These are important to remove as well and can be handled similarly. The exception here is that the `string.punctuation` variable will not often contain these, you'll have to tweak this to capture those characters as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-R-yAGWx57c-"
      },
      "source": [
        "##5.5. Token Replacement\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T01wNqCt6BIp"
      },
      "source": [
        "For certain tasks you may determine that some tokens need to be replaced with others. For instance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJgfwxkX6NRu"
      },
      "source": [
        "# Displays tokenization of contractions\n",
        "nltk.tokenize.word_tokenize(\"Aaron's out today.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHthGNS-6Uh0"
      },
      "source": [
        "We know that \"'s\" stands for \"is\", but we may want for the text we analyze to contain \"is\" rather than the abbreviation. Similarly, in some cases you may want to:\n",
        "* replace \"not happy\" with \"happy_NEG\" to indicate that the word was negated.\n",
        "* replace abbreviations with their expanded form.\n",
        "* replace emoticons with a textual representation of the emoticon\n",
        "\n",
        "These are cases when you would use token replacement. Consider the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fnV-0_s705k"
      },
      "source": [
        "# Displays token replacement in different stages\n",
        "sentence = \"Aaron's at the IT lab today and isn't expected to return until tomorrow\"\n",
        "list_of_words = nltk.tokenize.word_tokenize(sentence)\n",
        "print(f\"Before processing: {list_of_words})\")\n",
        "\n",
        "translation_dict = {\"'s\": \"is\",\n",
        "                    \"n't\": \"not\",\n",
        "                    \"IT\": \"Information Technology\"}\n",
        "\n",
        "list_of_words = [word if word not in translation_dict else translation_dict[word] for word in list_of_words]\n",
        "print(f\"\\nAfter first-round processing (abbreviation/contraction expansion): {list_of_words})\")\n",
        "\n",
        "list_of_words = nltk.sentiment.util.mark_negation(list_of_words)\n",
        "print(f\"\\nAfter second-round processing (negation marking): {list_of_words})\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOMyovtw-5cy"
      },
      "source": [
        "However, we have to be careful with token replacement lest we introduce artifacts in our text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCkg2u9J_FEN"
      },
      "source": [
        "# Displays token replacement for text that becomes invalid with the replacement operation.\n",
        "sentences = \"Let's meet at the lab tomorrow for brainstorming. IT IS IMPORTANT TO ARRIVE EARLY! \"\\\n",
        "            \"It's not a bad idea to keep the creative juices flowing\"\n",
        "list_of_words = nltk.tokenize.word_tokenize(sentences)\n",
        "print(f\"Before processing: {list_of_words})\")\n",
        "\n",
        "translation_dict = {\"'s\": \"is\",\n",
        "                    \"n't\": \"not\",\n",
        "                    \"IT\": \"Information Technology\"}\n",
        "\n",
        "list_of_words = [word if word not in translation_dict else translation_dict[word] for word in list_of_words]\n",
        "print(f\"\\nAfter first-round processing (abbreviation/contraction expansion): {list_of_words})\")\n",
        "\n",
        "list_of_words = nltk.sentiment.util.mark_negation(list_of_words)\n",
        "print(f\"\\nAfter second-round processing (negation marking): {list_of_words})\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHsNAQFs_mde"
      },
      "source": [
        "Here we can see that nuances in natural language led our rather brutish token replacement operation into creating problems.\n",
        "\n",
        "*   \"Let's\" was replaced with \"Let\" \"is\"\n",
        "*   \"IT\" (not referring to information technology) was replaced with \"Information Technology\"\n",
        "*   Words like 'creative' were negated because they followed 'not'; however, the negation clause shouldn't have negated those words.\n",
        "\n",
        "If you are going to use token replacement, do so with the utmost care and review the results to ensure that the desired output was obtained.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qC7kPAKpFr08"
      },
      "source": [
        "##5.6. Stop Word Removal\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9jiCqd5Fy7S"
      },
      "source": [
        "Some words are used so frequently in natural language as to be uninformative for some text analyses. These are commonly called 'stop words'. Let's take a look at some common stop words in English:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yaqdXESGxF-"
      },
      "source": [
        "# Displays the NLTK stop words list\n",
        "stop_words = nltk.corpus.stopwords.words(\"english\")\n",
        "print(stop_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k03WWMECG8RJ"
      },
      "source": [
        "For many analyses, these words do not tell us much regarding what is being talked about. Accordingly, in text analysis these stop words are sometimes removed as part of preprocessing.\n",
        "\n",
        "Let's take a look at an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qs6vE4d-HXqt"
      },
      "source": [
        "# Displays a tokenized sentence before and after stop words are removed\n",
        "sentence = \"I tend to agree with you that the lack of a liquid secondary market is a critical lynchpin \"\\\n",
        "           \"missing from equity crowdfunding taking off.\"\n",
        "sentence = nltk.word_tokenize(sentence.lower())\n",
        "print(f\"Before stop word removal: {sentence}\")\n",
        "\n",
        "sentence = [word for word in sentence if word not in stop_words]\n",
        "print(f\"After stop word removal: {sentence}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qBNIPL2Hs6W"
      },
      "source": [
        "We can still largely understand the meaning of the sentence without the stop words being present, which is a key idea behind stop word removal. However, [psychology research](https://www.secretlifeofpronouns.com/) also indicates that some of these words contain more meaning than we might imagine, suggesting that stop word removal should be done with care and only when doing so improves the validity/reliability of the analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmbscp-4JHKh"
      },
      "source": [
        "##5.7. Stemming/Lemmatization\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "va4pNn7aJMQD"
      },
      "source": [
        "Stemming and lemmatization are two approaches to accomplishing the same thing: reducing the number of morphemes in our corpus. However, that sounds complicated... let's simplify with an example:\n",
        "\n",
        "Do we lose much meaning by replacing \"*The innovative innovator innovatively created innovations in the innovation labs*\" with \"*The innovate innovate innovate create innovate innovate in the innovate lab*\"?\n",
        "\n",
        "Technically, the answer is 'yes'; however, if we're just trying to identify whether the sentence talks about innovation, both sentences convey about the same thing; however, the latter sentence is much simpler computationally because it contains only six distinct words (aka 'types') instead of ten.\n",
        "\n",
        "Let's look at an example of stemming:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUQr0xVhLfz3"
      },
      "source": [
        "# Displays a sentence before and after using the Porter stemmer\n",
        "sentence = \"I am enjoying my frequent vacations to beautiful San Diego\"\n",
        "sentence = nltk.word_tokenize(sentence.lower())\n",
        "print(f\"Before stemming: {sentence}\")\n",
        "\n",
        "stemmer = nltk.stem.porter.PorterStemmer()\n",
        "sentence = [stemmer.stem(word) for word in sentence]\n",
        "print(f\"After stemming: {sentence}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MV9LvCtgNuLQ"
      },
      "source": [
        "That works, but we're left with snippets of words rather than what we'd think to be the root of the word itself. That's a characterization of stemming.\n",
        "\n",
        "Let's look at Lemmatization:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS5Uo-J1OAMG"
      },
      "source": [
        "# Displays a sentence before and after using the WordNet Lemmatizer\n",
        "sentence = nltk.word_tokenize(\"I am enjoying my frequent vacations to beautiful San Diego\".lower())\n",
        "print(f\"Before lemmatization: {sentence}\")\n",
        "\n",
        "pos_map = {'J': wn.ADJ, 'N': wn.NOUN, 'R': wn.ADV, 'V': wn.VERB}\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "sentence = [lemmatizer.lemmatize(a, pos_map.get(b[0], wn.NOUN)) for a, b in nltk.pos_tag(sentence)]\n",
        "\n",
        "print(f\"After lemmatization: {sentence}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sexSsv4ucGIv"
      },
      "source": [
        "It's far from perfect. WordNet is built on a manually developed and maintained dataset, so not all words are in there and it is difficult to keep up-to-date with the constant shifts in natural language use. However, when it does work, you're left with a real word (e.g., \"vacation\") rather than a truncated word (e.g., \"vacat\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw8QDee3xYF8"
      },
      "source": [
        "#5.8. Preprocessing our Corpus\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tjxuggh5civm"
      },
      "source": [
        "Now that we've seen a litany of preprocessing activities. Let's put them to work in our corpus of About Us webpages and press releases:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dY34zooi0vM"
      },
      "source": [
        "# Load texts\n",
        "texts = [] \n",
        "for directory in dirs_to_load:\n",
        "  for file in glob.glob(f\"{directory}/*.txt\"):\n",
        "    with open(file, 'r') as infile: \n",
        "      text_type = file.split(\"/\")[-2]\n",
        "      text_id = file.split(\"/\")[-1]\n",
        "      texts.append({'text_type': text_type, 'text_id': text_id, 'text': infile.read()})\n",
        "\n",
        "# Text Preprocessing Pipeline\n",
        "for id, article in enumerate(texts):\n",
        "  if id == 0:\n",
        "    print(\"---Original--\")\n",
        "    print(article['text'][0:367])\n",
        "\n",
        "  # Segmentation\n",
        "  article['text'] = nltk.tokenize.sent_tokenize(article['text'])\n",
        "  if id == 0:\n",
        "    print(\"\\n---After segmentation--\")\n",
        "    print(article['text'][0:2])  \n",
        "\n",
        "  # Tokenization\n",
        "  article['text'] = [nltk.tokenize.word_tokenize(sentence) for sentence in article['text']]\n",
        "  if id == 0:\n",
        "    print(\"\\n---After tokenization--\")\n",
        "    print(article['text'][0:2])  \n",
        "\n",
        "  # Case conversion\n",
        "  for sent_id, sentence in enumerate(article['text']):\n",
        "    article['text'][sent_id] = [word.lower() for word in sentence]\n",
        "  if id == 0:\n",
        "    print(\"\\n---After case conversion--\")\n",
        "    print(article['text'][0:2])  \n",
        "\n",
        "  # Non-word character removal\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  for sent_id, sentence in enumerate(article['text']):\n",
        "    article['text'][sent_id] = [word.translate(table) for word in sentence if word.translate(table) and not word.isdigit()]\n",
        "  if id == 0:\n",
        "    print(\"\\n---After non-word character removal--\")\n",
        "    print(article['text'][0:2])\n",
        "\n",
        "  # Token replacement\n",
        "  translation_dict = {\"'s\": \"is\",\n",
        "                      \"n't\": \"not\",\n",
        "                      \"IT\": \"Information Technology\"}\n",
        "  for sent_id, sentence in enumerate(article['text']):\n",
        "    article['text'][sent_id] = [word if word not in translation_dict else translation_dict[word] for word in sentence]\n",
        "    article['text'][sent_id] = nltk.sentiment.util.mark_negation(sentence)\n",
        "  if id == 0:\n",
        "    print(\"\\n---After token replacement removal--\")\n",
        "    print(article['text'][0:2])\n",
        "\n",
        "  # Stop word removal\n",
        "  stop_words = nltk.corpus.stopwords.words(\"english\")\n",
        "  for sent_id, sentence in enumerate(article['text']):\n",
        "    article['text'][sent_id] = [word for word in sentence if word not in stop_words]\n",
        "  if id == 0:\n",
        "    print(\"\\n---After stop word removal--\")\n",
        "    print(article['text'][0:2])\n",
        "\n",
        "  # Lemmatization\n",
        "  pos_map = {'J': wn.ADJ, 'N': wn.NOUN, 'R': wn.ADV, 'V': wn.VERB}\n",
        "  lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "  for sent_id, sentence in enumerate(article['text']):\n",
        "    article['text'][sent_id] = [lemmatizer.lemmatize(a, pos_map.get(b[0], wn.NOUN)) for a, b in nltk.pos_tag(sentence)]\n",
        "  if id == 0:\n",
        "    print(\"\\n---After lemmatization--\")\n",
        "    print(article['text'][0:2])\n",
        "\n",
        "corpus_df = pd.DataFrame(texts)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGeAUkh7UQb3"
      },
      "source": [
        "The text preprocessing is complete - let's see our dataframe with the final preprocessed texts:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WfiDhqudyCl"
      },
      "source": [
        "# Display the first five rows of the dataframe\n",
        "display(corpus_df.head(5))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}