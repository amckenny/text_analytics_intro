{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "09_parts_of_speech.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNiGhjDMNCtXxy93IT6TVZP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amckenny/text_analytics_intro/blob/main/notebooks/09_parts_of_speech.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JowkrL3j6It_"
      },
      "source": [
        "#Prerequisites\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6ffY26d55PD"
      },
      "source": [
        "# Load 3rd-party packages\n",
        "!pip install --quiet stanza\n",
        "\n",
        "# Import Standard Library packages\n",
        "import glob\n",
        "from collections import Counter\n",
        "from IPython.display import display\n",
        "from pathlib import Path\n",
        "\n",
        "# Import 3rd-party packages\n",
        "import nltk, stanza\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4Jc1XmF6NG2"
      },
      "source": [
        "# Data Prerequisites\n",
        "!mkdir -p texts\n",
        "\n",
        "# Spin up Stanza\n",
        "stanza.download(\"en\")\n",
        "nlp = stanza.Pipeline('en', processors='tokenize,pos,ner', tokenize_no_ssplit=True)\n",
        "\n",
        "# Get text files\n",
        "!wget -q https://www.dropbox.com/s/5ibk0k4mibcq3q6/AussieTop100private.zip?dl=1 -O ./texts/AussieTop100private.zip\n",
        "!unzip -qq -n -d ./texts/ ./texts/AussieTop100private.zip\n",
        "\n",
        "about_dir = Path.cwd() / \"texts\" / \"About\"\n",
        "pr_dir = Path.cwd() / \"texts\" / \"PR\"\n",
        "dirs_to_load = [about_dir, pr_dir]\n",
        "\n",
        "# Text preprocessing prerequisites\n",
        "texts = [] \n",
        "for directory in dirs_to_load:\n",
        "  for file in glob.glob(f\"{directory}/*.txt\"):\n",
        "    with open(file, 'r') as infile: \n",
        "      text_type = file.split(\"/\")[-2]\n",
        "      text_id = file.split(\"/\")[-1]\n",
        "      fulltext = infile.read().replace(\"\\n\", \" \")\n",
        "      texts.append({'text_type': text_type, 'text_id': text_id, 'text': fulltext})\n",
        "text_df = pd.DataFrame(texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mB6kKZM87nM0"
      },
      "source": [
        "#Module 9 - Parts of Speech\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PieB25Zw7sn9"
      },
      "source": [
        "Thus far in the series he haven't much cared about linguistic characteristics of the words we've analyzed. Our analyses have been agnostic to whether we're looking at a noun, verb, adjective, or otherwise. For many purposes, that's OK... in some analyses, that isn't important.\n",
        "\n",
        "However, in some cases we would like to know what actions are being taken by individuals or organizations, requiring an understanding of verb phrases. Or we might be interested in how organizations view their salient stakeholders, requiring an understanding of proper nouns. In this module, we will demonstrate tools to help us tag parts of speech in our texts.\n",
        "\n",
        "The goals for this module are to:\n",
        "* Tag the parts of speech for individual words\n",
        "* Identify phrases within tagged text\n",
        "* Identify named entities in text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7Vjdarm-8cP"
      },
      "source": [
        "##9.1. Part-of-Speech Tagging\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8RPyKsN_ARU"
      },
      "source": [
        "If you have been taking a peek at the *prerequisites* text in the past few modules, you've probably seen that I have been doing a little bit of part-of-speech tagging behind the scenes without telling you. Specifically, when I have been preprocessing the texts for use in the module, I've used this line of code:\n",
        "\n",
        "`tokens = [word.text.lower() for sentence in nlp(fulltext).sentences for word in sentence.words if word.upos not in [\"PUNCT\", \"SYM\", \"NUM\", 'X']]`\n",
        "\n",
        "This line of code takes a text and does the following things:\n",
        "* Segments it into sentences\n",
        "* Tokenizes the sentences\n",
        "* Removes punctuation, symbols, numerals, and other non-text characters,\n",
        "* Changes the casing to lowercase\n",
        "* Saves the tokens to a list called 'tokens'\n",
        "\n",
        "But how does it know what tokens aren't words? Part-of-speech tagging. Each token is assigned a tag indicating its part of speech, including tokens that aren't words. There are many tools for accomplishing part-of-speech (POS) tagging; however, we're going to use Stanza - a Python NLP package associated with the Stanford NLP group in this module.\n",
        "\n",
        "Let's start with an example:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqQ7gwzJBkkn"
      },
      "source": [
        "# Tags a sentence for different parts of speech\n",
        "sentence = \"The greatest glory in living lies not in never falling, but in rising every time we fall.\"\n",
        "print(f\"Original sentence: {sentence}\")\n",
        "\n",
        "doc = nlp(sentence)\n",
        "print(f\"Tagged sentence:   {[(word.text, word.upos) for sentence in doc.sentences for word in sentence.words]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHLH5fDnDBkO"
      },
      "source": [
        "But is that all that impressive? Couldn't we just have one massive table that looks up the different parts of speech of different words?\n",
        "\n",
        "The answer: kind-of... but probably not. There's a couple of problems with that approach:\n",
        "\n",
        "* Natural language is constantly evolving, new words are being created every day - whose job it is to identify those new words and add them to our look-up table... and is that even feasible?\n",
        "* Some words change their part of speech depending on their usage.\n",
        "\n",
        "Consider the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Djh4C0p3Dn5H"
      },
      "source": [
        "# Tags the POS in a sentence using one word in two different ways\n",
        "sentence = \"I wouldn't bank on the bank being open on Friday.\"\n",
        "print(f\"Original sentence: {sentence}\")\n",
        "\n",
        "doc = nlp(sentence)\n",
        "print(f\"Tagged sentence:   {[(word.text, word.upos) for sentence in doc.sentences for word in sentence.words]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPK3UaW8D8mo"
      },
      "source": [
        "Note how the word \"bank\" is in there twice, but is being used both as a verb and as a noun? Our POS tagger recognizes the different usages and is able to tag it correctly.\n",
        "\n",
        "The reason this works is that Stanza uses a neural network to tag words rather than looking up the part of speech in a table. Let's see how it handles made-up words and misspellings. Let's compare then-president Donald Trump's famously misspelled tweet \"Despite the constant negative press covfefe\" to the correct version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32byREVJFpt5"
      },
      "source": [
        "# Compares the POS tagging of two parallel sentences, where one has a misspelling\n",
        "incorrect_sentence = \"Despite the constant negative press covfefe.\"\n",
        "correct_sentence = \"Despite the constant negative press coverage.\"\n",
        "inc_doc = nlp(incorrect_sentence)\n",
        "cor_doc = nlp(correct_sentence)\n",
        "print(f\"Incorrect sentence:   {[(word.text, word.upos) for sentence in inc_doc.sentences for word in sentence.words]}\")\n",
        "print(f\"Correct sentence:     {[(word.text, word.upos) for sentence in cor_doc.sentences for word in sentence.words]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0i9yeX7GJwt"
      },
      "source": [
        "Feel free to change 'covfefe' to any other random letters, I suspect you'll find the neural POS tagger still identifies it as a noun. It doesn't know the meaning of 'covfefe', but it can tell what it should be tagged based on how it is used in a sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ot1H0zN6feWg"
      },
      "source": [
        "##9.2. Chunking\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-CNtq_qfmfZ"
      },
      "source": [
        "Simply tagging the part of speech of each word can be helpful on its own, but we can learn a lot more about the text when we start looking at *chunks*, or groupings of tokens identified on the basis of their parts of speech.\n",
        "\n",
        "For instance, let's start by looking at noun phrases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ohSc3zCX6Xh"
      },
      "source": [
        "# Defines a function to chunk noun phrases from the passed POS tagged document\n",
        "def extract_noun_phrases(tagged_doc):\n",
        "  chunk_grammar = r\"NP: {(<DET>|<PRON>)?(<ADJ>)*(<CCONJ><ADJ>)?(<NOUN>|<PROPN>)+}\"\n",
        "  parser = nltk.RegexpParser(chunk_grammar)\n",
        "  parsed_doc = parser.parse(tagged_doc)\n",
        "  noun_phrases = []\n",
        "  for subtree in parsed_doc.subtrees(lambda x: x.label() == \"NP\"):\n",
        "    noun_phrases.append(\" \".join([word[0] for word in subtree.leaves()]))\n",
        "  return noun_phrases\n",
        "\n",
        "# Extracts noun phrases from a sample text\n",
        "sentence = \"The brown cow called out to the happy yet tired farmer.\"\n",
        "tagged_doc = [(word.text, word.upos) for sentence in nlp(sentence).sentences for word in sentence.words]\n",
        "print(f\"Noun Phrases: {extract_noun_phrases(tagged_doc)}\")   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvxsk1sBtxwI"
      },
      "source": [
        "The `chunk_grammar` variable contains our 'rules' for chunking noun phrases. Here we're using [regular expressions](https://docs.python.org/3/howto/regex.html) which go far beyond the scope of this module. However, in this case the regular expression looks for:\n",
        "* A determiner (e.g., \"the\") or pronoun (\"his\"), if there is one\n",
        "* Adjectives (e.g., \"green\"), followed by a conjunction and adjective (e.g., \"and yellow\"), if there are any\n",
        "* At least one noun or proper noun\n",
        "\n",
        "This clearly doesn't capture all the ways that noun phrases can be modified. For instance, noun phrases can use postmodifiers as in \"the man with a green tattoo.\" However, this will work for our illustrative example.\n",
        "\n",
        "The *prerequisites* code preloaded some 'about us' webpages and press release texts. Let's take a look at noun phrases from the first text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d34MNA3Stu8N"
      },
      "source": [
        "# Extracts verb phrases from a text in the loaded corpus\n",
        "text_to_extract_from = 0\n",
        "tagged_doc = [(word.text, word.upos) for sentence in nlp(text_df.iloc[text_to_extract_from][\"text\"]).sentences \n",
        "                                     for word in sentence.words]\n",
        "noun_phrases = extract_noun_phrases(tagged_doc)\n",
        "print(f\"Noun Phrases\\n{'-'*30}\")\n",
        "for phrase in sorted(set(noun_phrases)):\n",
        "  print(phrase)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e53ldm_ExzmE"
      },
      "source": [
        "Chunking can be used to capture a wide variety of patterns of speech beyond noun phrases. For instance, we can capture prepositional phrases and verb phrases as well. \n",
        "\n",
        "Verb phrases may be particularly valuable to understand what is happening in a text. Let's take a look at how we would change the chunk grammar to capture verb phrases:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mowDEH8Ix28R"
      },
      "source": [
        "# Defines a function to chunk verb phrases from the passed POS tagged document\n",
        "def extract_verb_phrases(tagged_doc):\n",
        "  chunk_grammar = r\"VP: {<AUX>*<VERB>+}\"\n",
        "  parser = nltk.RegexpParser(chunk_grammar)\n",
        "  parsed_doc = parser.parse(tagged_doc)\n",
        "  verb_phrases = []\n",
        "  for subtree in parsed_doc.subtrees(lambda x: x.label() == \"VP\"):\n",
        "    verb_phrases.append(\" \".join([word[0] for word in subtree.leaves()]))\n",
        "  return verb_phrases\n",
        "\n",
        "# Extracts verb phrases from a sample text\n",
        "sentence = \"I should have gone to to the doctor because I became very sick\"\n",
        "tagged_doc = [(word.text, word.upos) for sentence in nlp(sentence).sentences for word in sentence.words]\n",
        "print(f\"Verb Phrases: {extract_verb_phrases(tagged_doc)}\")   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAfr_U-K1LR3"
      },
      "source": [
        "Here we're looking for:\n",
        "* One or more auxiliary verbs (if they occur at all), followed by \n",
        "* the main verb\n",
        "\n",
        "Here too, verb phrases can be more complex than this simple pattern; however, for our purposes this will suffice.\n",
        "\n",
        "Let's look at the verb patterns in a real text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHJrEJac1PE1"
      },
      "source": [
        "# Extracts verb phrases from a text in the loaded corpus\n",
        "text_to_extract_from = 0\n",
        "tagged_doc = [(word.text, word.upos) for sentence in nlp(text_df.iloc[text_to_extract_from][\"text\"]).sentences \n",
        "                                     for word in sentence.words]\n",
        "verb_phrases = extract_verb_phrases(tagged_doc)\n",
        "print(f\"Verb Phrases\\n{'-'*30}\")\n",
        "for phrase in sorted(set(verb_phrases)):\n",
        "  print(phrase)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNzCUVyt1f3V"
      },
      "source": [
        "##9.3. Named Entity Recognition\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCR1J1sHW8ce"
      },
      "source": [
        "Proper nouns may be of particular interest to our research. For instance, understanding the salient stakeholders of an organization based on the attention they receive in a corpus could be done by using chunking (as above) to select all proper nouns. However, this area is important enough in natural language processing research that it has evolved in parallel to part-of-speech tagging.\n",
        "\n",
        "Named entity recognition seeks to extract and classify the names of people places and things in a text. For example, \"William Henry Gates III, founder and former CEO of Microsoft Corporation was born in Seattle, Washington.\" has several named entities:\n",
        "* William Henry Gates III - the person\n",
        "* Microsoft Corporation - the company\n",
        "* Seattle, Washington - the location\n",
        "\n",
        "Let's have Python try to identify the named entities in this sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bSwEUKBasAN"
      },
      "source": [
        "# Extracts named entities from a sample text\n",
        "sentence = \"William Henry Gates III, founder and former CEO of Microsoft Corporation was born in Seattle, Washington.\"\n",
        "entities = nlp(sentence).entities\n",
        "for entity in entities:\n",
        "  print(f\"{entity.text:25} is a {entity.type}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSmOgrIJcPbN"
      },
      "source": [
        "That worked. It identified the three named entities (though it split Seattle and Washington into two) and it correctly identified the type of entity they are (GPE stands for geopolitical entity).\n",
        "\n",
        "Let's try it on a full text from our sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EU89X57-dcOs"
      },
      "source": [
        "# Extracts named entities from a text in the loaded corpus\n",
        "text_to_extract_from = 0\n",
        "entities = nlp(text_df.iloc[text_to_extract_from][\"text\"]).entities\n",
        "for entity in entities:\n",
        "  print(f\"{entity.text:60} - is a {entity.type}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuwcMYcIePgq"
      },
      "source": [
        "Ok, let's do something a bit more applied with this. Let's write some code that:\n",
        "* Identifies all organizations in our corpus\n",
        "* Counts the number of times each organization is mentioned\n",
        "* Presents the top ten such organizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBeRShEfePIz"
      },
      "source": [
        "# Have Stanza analyze our corpus of texts\n",
        "text_df['nlp'] = text_df['text'].apply(nlp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oV5X5EG3fbXT"
      },
      "source": [
        "# Count all organizations mentioned in each text\n",
        "def gen_org_counter(doc):\n",
        "  org_counter = Counter()\n",
        "  for entity in doc.entities:\n",
        "    if entity.type == \"ORG\":\n",
        "      org_counter[entity.text]+=1\n",
        "  return org_counter\n",
        "\n",
        "text_df['organizations'] = text_df['nlp'].apply(gen_org_counter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ux1BHqysj3-1"
      },
      "source": [
        "# Sum the organization counts for all texts and print the ten most common\n",
        "corpus_counter = text_df['organizations'].sum()\n",
        "print(corpus_counter.most_common(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzFM_c1EkJKj"
      },
      "source": [
        "It did a pretty good job. We do see some artifacts (i.e., \"Company\" and \"Group\") indicating that the analysis isn't perfect; however, this is still a pretty informative list. We could now slice and dice this dataset several ways to understand whether these are companies talking about themselves or others, or we could compare who is being talked about in different types of texts.\n",
        "\n",
        "In fact, let's do that, let's compare the organizations being discussed in 'About Us' webpages vs press releases:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_YGubnwk2L7"
      },
      "source": [
        "grouped_df = text_df.groupby(['text_type'])\n",
        "corpus_counter = grouped_df['organizations'].sum()\n",
        "for text_type, type_counter in corpus_counter.items():\n",
        "  print(f\"Text type: {text_type}\\nOrganization mentions: {type_counter.most_common(10)}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhWURi_VmwIa"
      },
      "source": [
        "The opportunities for the application of these techniques in organizational research abound. \n",
        "\n",
        "For avid readers: [Coreference resolution](https://nlp.stanford.edu/projects/coref.shtml) is a complementary area of natural language processing which helps identify all references to an entity throughout the text, but is beyond the scope of this series."
      ]
    }
  ]
}