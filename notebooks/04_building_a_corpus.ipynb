{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_building_a_corpus.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNMzRBxRKW6i4aM+wjtmJAM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amckenny/text_analytics_intro/blob/main/notebooks/04_building_a_corpus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GlbytuO_o-r"
      },
      "source": [
        "#Prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2XsAIe587HA"
      },
      "source": [
        "# Get external files and install 3rd party packages\n",
        "!mkdir -p texts\n",
        "!mkdir -p texts/10ks\n",
        "!wget -q https://www.dropbox.com/s/5ibk0k4mibcq3q6/AussieTop100private.zip?dl=1 -O ./texts/AussieTop100private.zip\n",
        "!wget -q https://www.dropbox.com/s/u6m4k0uhhj9m2um/Sample_Qualtrics_Output.xlsx?dl=1 -O ./texts/Sample_Qualtrics_Output.xlsx\n",
        "!unzip -qq -d ./texts/ ./texts/AussieTop100private.zip\n",
        "!pip install -U sec-edgar-downloader\n",
        "\n",
        "# Standard library imports\n",
        "import glob, pprint, random, requests, time\n",
        "from pathlib import Path\n",
        "from IPython.display import display\n",
        "\n",
        "# 3rd party imports\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from sec_edgar_downloader import Downloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKQ-ksuD_1uf"
      },
      "source": [
        "#Module 4 - Building a Corpus\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlQMH3o7_7U8"
      },
      "source": [
        "One of the most time consuming aspects of text analysis is actually building the corpus of texts themselves. With the increasing availability of texts in electronic format over the Internet, this is getting increasingly easier and faster. However, it is still far from a trivial process.\n",
        "\n",
        "In this module, we'll introduce several methods of obtaining and getting texts into Python for analysis. The goals for this module are:\n",
        "\n",
        "* Load a corpus from text files.\n",
        "* Load a corpus from Qualtrics survey exports.\n",
        "* Load a corpus from an API.\n",
        "* Load a corpus from web scraping.\n",
        "\n",
        "**Note**: Whereas modules 1-3 were designed to be used in a self-directed manner, modules 4 and on are designed to be part of my workshop/course. There is far less prose explanation in these notebooks. However, with some tinkering you may still be able to work through these on your own."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YszE3YarJ7Ot"
      },
      "source": [
        "##4.1. Building a Corpus from Text Files\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHbFCH9wKW1G"
      },
      "source": [
        "The *prerequisites* code automatically loaded two text corpora into the './texts/About/' and './texts/PR/' directories. \n",
        "\n",
        "Go to the file navigator in Colab (on the left, it looks like a folder) and verify that they're there. If they're not there, ensure that you ran the prerequisites code above and click the refresh button just under \"Files\" in the file navigator (looks like a folder with a circle at the bottom-right).\n",
        "\n",
        "If you don't see these folders here, neither will Python!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CNJ7qOaPSST"
      },
      "source": [
        "We want to load every '.txt' file in those directories, so first we need to tell Python where those directories are."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q08XhiQWPm4Y"
      },
      "source": [
        "# Tell Python directories where texts are located\n",
        "texts_dir = Path.cwd() / \"texts\"\n",
        "about_dir =  texts_dir / \"About\" \n",
        "pr_dir = texts_dir / \"PR\" \n",
        "\n",
        "dirs_to_load = [about_dir, pr_dir]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIc3V5LOQCMc"
      },
      "source": [
        "We will then create two loops to get all the files in the directories:\n",
        "\n",
        "1. Loop through all of the directories we want to load texts from (`dirs_to_load`)\n",
        "\n",
        "2. Loop through all .txt files in that directory\n",
        "\n",
        "Then for each "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uabTVGJeRXQn"
      },
      "source": [
        "# Loads the texts into a list called \"texts\"\n",
        "texts = [] \n",
        "for directory in dirs_to_load: # Loop 1\n",
        "  for file in glob.glob(f\"{directory}/*.txt\"): # Loop 2\n",
        "    with open(file, 'r') as infile: # Open the text file\n",
        "      text_type = file.split(\"/\")[-2]\n",
        "      text_id = file.split(\"/\")[-1]\n",
        "      texts.append({'text_type': text_type, 'text_id': text_id, 'text': infile.read()}) # Save the contents of the file to the \"texts\" list\n",
        "\n",
        "# Creates a Pandas DataFrame from the corpus and saves the Dataframe as a .csv file\n",
        "corpus_df = pd.DataFrame(texts) \n",
        "corpus_df.to_csv(texts_dir / \"about_pr_texts.csv\")\n",
        "\n",
        "# Displays information about the corpus\n",
        "print(f\"There are {len(corpus_df)} texts in the corpus\")\n",
        "print(\"\\nThe number of each type of text:\")\n",
        "display(corpus_df.groupby(by='text_type').agg('count')['text'])\n",
        "print(\"\\nA sample of what's in the table:\")\n",
        "display(corpus_df.head(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB3JUPf4VUq0"
      },
      "source": [
        "That's it! There are ways of getting word files/PDF files/etc into Python as well. They follow a similar pattern, but are beyond the scope of this module."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DR1pB8CAYPPs"
      },
      "source": [
        "##4.2. Building a Corpus from Qualtrics\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jc5yQp-MYfEV"
      },
      "source": [
        "Another source of texts you may want to analyze is results from free-response questions in a survey/experiment. Often we are able to export the full results from our survey instrument; however, it'd be nice not to have to break the spreadsheet into separate text documents. Let's see how this can be done using the Excel exported by Qualtrics.\n",
        "\n",
        "First, we tell Python the name and location of the Qualtrics export file. In this case we use Excel format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1J8yHvob3M0"
      },
      "source": [
        "# Tells Python where to find the qualtrics export file\n",
        "qualtrics_file = texts_dir / \"Sample_Qualtrics_Output.xlsx\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdMn9-eecD15"
      },
      "source": [
        "Once Python knows where the Wualtrics file can be found, one line of code imports that file into a Pandas DataFrame.\n",
        "\n",
        "From there we probably want to drop the first row (Qualtrics has two header rows and by default Pandas reads one of them in as 'data'). We also may want to save only certain columns of relevance to our analysis (in this case we'll save only the Progress, Duration, and text) to make viewing the output easier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54qji5kXcEMR"
      },
      "source": [
        "# Load the Qualtrics export file into a Pandas DataFrame\n",
        "qualtrics_df = pd.read_excel(qualtrics_file, header=0)\n",
        "\n",
        "# Eliminates unneeded rows/columns and saves result to a csv file\n",
        "qualtrics_df = qualtrics_df.drop(0)\n",
        "qualtrics_df = qualtrics_df[[\"Progress\",\"Duration (in seconds)\", \"sample_Text\"]]\n",
        "qualtrics_df.to_csv(texts_dir / \"qualtrics_texts.csv\")\n",
        "\n",
        "# Displays the first five rows of the DataFrame\n",
        "display(qualtrics_df.head(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmUmcguEgrGv"
      },
      "source": [
        "As with loading text files, there are also multiple ways of loading survey data into Python (e.g., with a csv file, etc). Here too, it follows a similar procedure, but is beyond the scope of this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpoYYOtkgpli"
      },
      "source": [
        "##4.3. Building a Corpus from an API\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZf3xsAlhCvR"
      },
      "source": [
        "Sometimes we want to collect texts from organizations who have built an 'API' or **A**pplication **P**rogramming **I**nterface to help us obtain texts. Myriad organizations have such APIs and each one works a little differently. \n",
        "\n",
        "Depending on the API, sometimes you can find Python code written by someone else that will make using the API easier. We're going to look at how to get 10-K documents from the SEC EDGAR database. As it turns out, [Jad Chaar](https://github.com/jadchaar/) has written some Python code we're going to use to make our lives easier: the [sec-edgar-downloader](https://github.com/jadchaar/sec-edgar-downloader) package.\n",
        "\n",
        "(The installation and loading of this package is done in the *Prerequisites* code.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_gSZeU9jdTL"
      },
      "source": [
        "According to his package documentation, we first need to create a Downloader object and tell it where to store the files.\n",
        "\n",
        "Notice that we're telling Python where to *store* them, not where to *find* them. Unlike the manual/qualtrics corpus examples, here we're getting the files from an online source and the API already knows where to find them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fI70l6gIjJWb"
      },
      "source": [
        "# Initializes the SEC downloader and tells it where to store the 10-Ks\n",
        "tenk_directory = Path.cwd() / \"texts\" / \"10ks\"\n",
        "dl = Downloader(tenk_directory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygZbGwNBkUZf"
      },
      "source": [
        "We then tell the Downloader object what files we want and from what company. Let's download **all** of IBM and Apple's 10-K documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdIYBXIzkmEu"
      },
      "source": [
        "# Tells the API we want the IBM and Apple 10-Ks\n",
        "company_tickers = [\"IBM\", \"AAPL\"]\n",
        "for ticker in company_tickers:\n",
        "  dl.get(\"10-K\", ticker)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfwDtQdwk9qP"
      },
      "source": [
        "Go over to the file navigator in Colab and look in the './texts/10ks' directory. You'll see that there is now an entire directory tree housing the downloaded texts. They're not all downloaded into one directory like we had when we used our own texts.\n",
        "\n",
        "You *could* copy and paste them all into one directory, but imagine if we had used a loop to get all 10-k documents for all S&P 500 companies. That would take forever! Let's use Python to go through these directories for us so we don't have to!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ec3a5ekmR28"
      },
      "source": [
        "# Identifies the companies based on the tickers in the sec-edgar-filings directory\n",
        "results_dir = tenk_directory / \"sec-edgar-filings\"\n",
        "companies = [company.name for company in results_dir.iterdir() if results_dir.is_dir()] \n",
        "\n",
        "texts = []\n",
        "\n",
        "# Loops through the directories and collects all of the 10-K information in a list called 'texts'\n",
        "for company in companies:\n",
        "  company_10k_dir = results_dir / company / \"10-K\"\n",
        "  filings = [filing.name for filing in company_10k_dir.iterdir() if company_10k_dir.is_dir()] \n",
        "\n",
        "  for filing_id in filings:\n",
        "    tenk_filename = company_10k_dir / filing_id / \"full-submission.txt\"\n",
        "    if tenk_filename.exists():\n",
        "      with open(tenk_filename, 'r') as infile:\n",
        "        texts.append({'company': company, 'filing_type': \"10-K\", 'filing_id': filing_id, 'text': infile.read()})\n",
        "\n",
        "# Converts the 'texts' list to a Pandas DataFrame and outputs it to a csv file\n",
        "tenk_df = pd.DataFrame(texts)\n",
        "tenk_df.to_csv(texts_dir / \"tenk_texts.csv\")\n",
        "\n",
        "# Displays information about the corpus\n",
        "print(f\"There are {len(tenk_df)} texts in the corpus\")\n",
        "print(\"\\nThe number of texts from each company:\")\n",
        "display(tenk_df.groupby(by='company').agg('count')['text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsqbHBSKqn1b"
      },
      "source": [
        "Now let's take a look at one of our texts and see what it looks like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fuJMJP9qsYe"
      },
      "source": [
        "# Displays the first 10,000 characters of one of the texts in the corpus\n",
        "print(tenk_df.iloc[-1]['text'][:10000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsdPTIXord5u"
      },
      "source": [
        "Well... that's certainly a 10-K... however, that's not all text. That almost looks like the code behind an HTML file! ...and yep, that's how you get it. It looks like texts from this data source are going to require some cleaning before we use them in a text analysis!\n",
        "\n",
        "Every API works a little bit differently, so you're often going to have to go to the API documentation and tinker a bit. However, once you have been through a few APIs, you'll generally see the same ideas implemented over and over again with a few tweaks from API to API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiqOx_h0wDVo"
      },
      "source": [
        "##4.4. Scraping a Corpus\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llNPlOvNxuF3"
      },
      "source": [
        "Sometimes the texts that you want are online and there is no API readily available to interface with. In this case, you're left with a decision: scrape the text or collect it manually.\n",
        "\n",
        "There are advantages and disadvantages of each, and we'll talk about that in the workshop/course. However, one thing I want to put in the notebook is an ethical concern. Some websites explicitly disallow scraping. I've observed some scholars scraping these sites (I suspect without permission), but I don't agree with this approach. Use this knowledge/these tools for good and where permitted.\n",
        "\n",
        "For our example, let's see what's going on at the [Kelley School of Business](https://news.iu.edu/tags/kelley-school-of-business). I searched through the site and didn't see anything prohibiting it, so it seems fair to use so long as we're mindful not to be too taxing on the system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmzQgmEuh-Iw"
      },
      "source": [
        "###4.4.1. Building the List of URLs to Scrape\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mq0yWDYwzNKH"
      },
      "source": [
        "First let's have Python go out and get the news page and see what we see:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ns9feiGz0C_H"
      },
      "source": [
        "# Has Python call the webpage with the article links on it and displays whether the page was accessed successfully\n",
        "url = \"https://news.iu.edu/tags/kelley-school-of-business\"\n",
        "response = requests.get(url)\n",
        "\n",
        "status = response.status_code\n",
        "if status == 200:\n",
        "  print(f\"The status code was {status} - that means that we received the webpage back\")\n",
        "else:\n",
        "  print(f\"The status code was {status} - something didn't work\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB5thp8Z6IgS"
      },
      "source": [
        "Now let's look at the \"text\" we got back:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYp1xtW11-tv"
      },
      "source": [
        "# Displays the contents of the webpage that was accessed\n",
        "text = response.text\n",
        "print(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgcIbiiR6YDY"
      },
      "source": [
        "Well that's massive, and again, in HTML... but if we [navigate to the website](https://news.iu.edu/tags/kelley-school-of-business) and compare what we see there to the code, a pattern emerges:\n",
        "\n",
        "* Each story we want to access seems to be contained in a tag called: `<div class=\"grid-item--container\">`\n",
        "\n",
        "This insight enables us to extract from the HTML only the bits that surround the news articles. We do so with BeautifulSoup:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDK0ORLB0NUQ"
      },
      "source": [
        "# Displays only the website text within the grid-item--container sections\n",
        "bs_text = BeautifulSoup(text)\n",
        "article_containers = bs_text.find_all('div', attrs={'class':'grid-item--container'})\n",
        "print(article_containers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJy9lhuDGcc1"
      },
      "source": [
        "What we want from here is just the URL to the articles. We see that within each container tag, the URLs are stored within an `<a href=...>` tag.\n",
        "\n",
        "Let's get just those."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viKA9TVd0flK"
      },
      "source": [
        "# Displays the URLs to the articles\n",
        "for article in article_containers:\n",
        "  print(article.a['href'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCFSaTSgT3U3"
      },
      "source": [
        "OK, so now we can see the URLs... but there appears to be two different kinds:\n",
        "* Stories: Start with /\n",
        "* Blog entries: Contain the full URL.\n",
        "\n",
        "We could do both, but that would require us to scrape two pages with two separate formats. Let's just do the stories for our demo.\n",
        "\n",
        "We know that the stories all start with https://news.iu.edu, so let's prepend that and add those to a list of all articles:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OnwRDTFU5kg"
      },
      "source": [
        "# Creates a list of URLs for the selected articles starting with a forward slash (/)\n",
        "article_urls = [\"https://news.iu.edu\"+article.a['href'] for article in article_containers if article.a['href'].startswith('/')]\n",
        "print(article_urls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raHYm3wGVd4J"
      },
      "source": [
        "That's great, but there's one more important piece of information... this isn't the last page of news stories... there are many more pages we need to get the links from. \n",
        "\n",
        "How can we tell this? Well if you look at the webpage, there is a \"Next >\" button when there are no more pages of news.\n",
        "\n",
        "If we look in our HTML, we see that there is a `<li class=\"next\">` tag when that button is there. Let's take a look at that:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IABlHz9YV1Gf"
      },
      "source": [
        "# Finds the HTML code for the 'next' button and prints it\n",
        "next_page_code = bs_text.find('li', attrs={'class':'next'})\n",
        "print(next_page_code)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmbuJEy5XgIj"
      },
      "source": [
        "It looks like it too has a URL in it within an `<a href=...>` tag... but this time starting with a question-mark. That just means at the end of \"https://news.iu.edu/tags/kelley-school-of-business' we need to add a question-mark and the page number like so:\n",
        "\n",
        "`https://news.iu.edu/tags/kelley-school-of-business?page=2`\n",
        "\n",
        "Let's get this URL as well so we know what page to get data from next:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBJtBQpqYPRE"
      },
      "source": [
        "# Extracts the URL from the HTML code for the 'next' button.\n",
        "next_news_url = \"https://news.iu.edu/tags/kelley-school-of-business\"+next_page_code.a['href']\n",
        "print(next_news_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4CdGLMHYmOa"
      },
      "source": [
        "Let's systematize what we've done a little bit:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GScCGzZPY8WH"
      },
      "source": [
        "def get_kelley_news_urls(url):\n",
        "  # Get URL Data\n",
        "  response = requests.get(url)\n",
        "  status = response.status_code\n",
        "  if status == 200:\n",
        "    print(f\"URL \\\"{url}\\\" successfully requested. Parsing...\", end=\" \")\n",
        "  else:\n",
        "    print(f\"URL \\\"{url}\\\" failed with code {status}. Skipping...\", end=\" \")\n",
        "    return ([], None)\n",
        "  html_code = response.text\n",
        "  bs_text = BeautifulSoup(html_code)\n",
        "\n",
        "  # Parse news URLs\n",
        "  article_containers = bs_text.find_all('div', attrs={'class':'grid-item--container'})\n",
        "  article_urls = [\"https://news.iu.edu\"+article.a['href'] for article in article_containers if article.a['href'].startswith('/')]\n",
        "\n",
        "  # Find \"Next\" button: Return URL if it's there or None if it isn't.\n",
        "  next_page_code = bs_text.find('li', attrs={'class':'next'})\n",
        "  print(\"Returning...\", end=\" \")\n",
        "  if next_page_code is None:\n",
        "    return (article_urls, None)\n",
        "  else:\n",
        "    next_news_url = \"https://news.iu.edu/tags/kelley-school-of-business\"+next_page_code.a['href']\n",
        "    return (article_urls, next_news_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1Xyl1k_cyxc"
      },
      "source": [
        "And let's see if it produces consistent results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFUh5DwWc48F"
      },
      "source": [
        "# Gets all article URLs and the 'next' URL from the specified webpage\n",
        "url = \"https://news.iu.edu/tags/kelley-school-of-business\"\n",
        "get_kelley_news_urls(url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZ8JtIUIe_uU"
      },
      "source": [
        "Ok, now let's use a loop to get **all** of the URLs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cF_amodufYo_"
      },
      "source": [
        "# Iteratively accesses the Kelley news page, extracting the article URLs and 'next' URL until there are no more articles to be extracted.\n",
        "url = \"https://news.iu.edu/tags/kelley-school-of-business\"\n",
        "list_of_article_urls = []\n",
        "while True:\n",
        "  result_tuple = get_kelley_news_urls(url)\n",
        "  list_of_article_urls.extend(result_tuple[0])\n",
        "  url = result_tuple[1]\n",
        "  if not url:\n",
        "    break\n",
        "  else:\n",
        "    print(f\"Sleeping...\")\n",
        "    time.sleep(3)\n",
        "\n",
        "print(f\"\\n\\nThe full list of articles is: {list_of_article_urls}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wn3iBgIpiF8V"
      },
      "source": [
        "###4.4.2. Scraping the News Articles\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPWFGGueiRcz"
      },
      "source": [
        "Now that we have a list of the URLs for the articles themselves, we will largely repeat what we did above. The difference is that here we are looking for the text of the article, not the URLs to be scraped.\n",
        "\n",
        "Let's start with one article, the first in our list:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xe0uPrdOinQh"
      },
      "source": [
        "url =  list_of_article_urls[0]\n",
        " \n",
        "# Get URL Data\n",
        "response = requests.get(url)\n",
        "html_code = response.text\n",
        "bs_text = BeautifulSoup(html_code)\n",
        "print(bs_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAz3zDyrimm3"
      },
      "source": [
        "And we're back to a mess of HTML again, but we see some valuable data in this HTML:\n",
        "\n",
        "*note*: You'll see I added 'try' and 'except' blocks here. This is because some articles may/may not have each field. If you try to access a field that doesn't exist, Python will throw an error at you. The 'try' and 'except's just tell Python what to do if there is no error ('try') and what to do if there is an error ('except')."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dp5HCeY2jYTX"
      },
      "source": [
        "# The category\n",
        "try:\n",
        "  category = bs_text.find('div', attrs={'class': 'article-category'}).a.text.strip()\n",
        "  print(category)\n",
        "except:\n",
        "  print(\"There was no category for this article\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMQ-GzI6kBWg"
      },
      "source": [
        "# The title \n",
        "title = bs_text.find('h1', attrs={'class': 'article--title'}).text.strip()\n",
        "print(title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nb1Fq7c6kxiF"
      },
      "source": [
        "# The subtitle\n",
        "try:\n",
        "  subtitle = bs_text.find('h2', attrs={'class': 'article--subtitle'}).text.strip()\n",
        "  print(subtitle)\n",
        "except:\n",
        "  print(\"There was no subtitle for this article\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmfQvf6klFu0"
      },
      "source": [
        "# The author\n",
        "try:\n",
        "  author = bs_text.find('p', attrs={'class': 'byline author'}).text.replace(\"By\\n\", \" \").strip()\n",
        "  print(author)\n",
        "except:\n",
        "  print(\"There was no author for this article\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ev9pZyFalZXK"
      },
      "source": [
        "# The date\n",
        "try:\n",
        "  date = bs_text.find('p', attrs={'class': 'byline date'}).text.strip()\n",
        "  print(date)\n",
        "except:\n",
        "  print(\"There was no date for this article\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06PAFC5XmQny"
      },
      "source": [
        "We also see the body of the text in the `<div class=\"text\">` tags. However, unlike the others, there are more than one of them, and they contain HTML tags in them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYawdkTRlz7q"
      },
      "source": [
        "# The text body\n",
        "body_text = bs_text.find_all('div', attrs={'class': 'text'})\n",
        "for section in body_text:\n",
        "  print(section)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4I5K2O6nFJG"
      },
      "source": [
        "Fortunately, BeautifulSoup has a `get_text()` function that will help us get only the printed text from this. We can stitch together the multiple sections ourselves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ibu4ok7LnUa5"
      },
      "source": [
        "# Extracts the displayed text from the HTML\n",
        "fulltext = \"\"\n",
        "for section in body_text:\n",
        "  fulltext = fulltext + \" \" + section.get_text().strip()\n",
        "\n",
        "print(fulltext)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E01rLdKJn6Jp"
      },
      "source": [
        "Again, let's pull this together into one function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyQK4aWjoBlz"
      },
      "source": [
        "def parse_kelley_news_page(url):\n",
        "  # Get URL Data\n",
        "  response = requests.get(url)\n",
        "  status = response.status_code\n",
        "  if status == 200:\n",
        "    print(f\"URL \\\"{url}\\\" successfully requested. Parsing...\", end=\" \")\n",
        "  else:\n",
        "    print(f\"URL \\\"{url}\\\" failed with code {status}. Skipping...\", end=\" \")\n",
        "    return (None)\n",
        "  html_code = response.text\n",
        "  bs_text = BeautifulSoup(html_code)\n",
        "\n",
        "  # Parse HMTL into article sections\n",
        "  article = {}\n",
        "  article[\"url\"] = url\n",
        "  \n",
        "  # Not every article will have every field, so we use 'try' and 'except' statements to handle cases when it does not\n",
        "  try:\n",
        "    article[\"title\"] = bs_text.find('h1', attrs={'class': 'article--title'}).text.strip()\n",
        "  except:\n",
        "    article[\"title\"] = \"None\"\n",
        "  try:\n",
        "    article[\"category\"] = bs_text.find('div', attrs={'class': 'article-category'}).a.text.strip()\n",
        "  except:\n",
        "    article[\"category\"] = \"None\"\n",
        "  try:\n",
        "    article[\"subtitle\"] = bs_text.find('h2', attrs={'class': 'article--subtitle'}).text.strip()\n",
        "  except:\n",
        "    article[\"subtitle\"] = \"None\"\n",
        "  try:\n",
        "    article[\"author\"] = bs_text.find('p', attrs={'class': 'byline author'}).text.replace(\"By\\n\", \" \").strip()\n",
        "  except:\n",
        "    article[\"author\"] = \"None\"\n",
        "  try:\n",
        "    article[\"date\"] = bs_text.find('p', attrs={'class': 'byline date'}).text.strip()\n",
        "  except:\n",
        "    article[\"date\"] = \"None\"\n",
        "  try:\n",
        "    body_text = bs_text.find_all('div', attrs={'class': 'text'})\n",
        "    article[\"fulltext\"] = \"\"\n",
        "    for section in body_text:\n",
        "      article[\"fulltext\"] = article[\"fulltext\"] + \" \" + section.get_text().strip()\n",
        "  except:\n",
        "    article[\"fulltext\"] = \"\"\n",
        "\n",
        "  print(\"Done...\", end=\" \")\n",
        "  return article"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrpjIPDEpJdJ"
      },
      "source": [
        "Let's see if it produces consistent results to what we saw previously:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Qy1XkkgpNF2"
      },
      "source": [
        "# Tests our custom parsing function to see if it pulls the right information\n",
        "result_dict = parse_kelley_news_page(list_of_article_urls[2])\n",
        "print(f\"\\n{result_dict}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xq4-JDbqp2gA"
      },
      "source": [
        "Now we will build a loop to take us through all of the article URLs we scraped:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQMayCRjp2Lz"
      },
      "source": [
        "article_texts = []\n",
        "\n",
        "# Iterates through all article URLs, extracting the article information and text, and stores it to a list 'article_texts'\n",
        "for url in list_of_article_urls:\n",
        "  result_dict = parse_kelley_news_page(url)\n",
        "  if result_dict:\n",
        "    article_texts.append(result_dict)\n",
        "  print(f\"Sleeping...\")\n",
        "  time.sleep(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vw-ClkOGzDdb"
      },
      "source": [
        "# Creates a Pandas DataFrame from the results and saves the corpus to a csv file\n",
        "kelleynews_df = pd.DataFrame(article_texts)\n",
        "kelleynews_df.to_csv(texts_dir/\"kelleynews_texts.csv\")\n",
        "\n",
        "# Displays the contents of the corpus\n",
        "display(kelleynews_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCQibGUMZ3v0"
      },
      "source": [
        "Now you have a corpus of Kelley-related news articles scraped from the IU webpage. If you go to the file navigator on the left, you can download the corpus from the server to your local machine. "
      ]
    }
  ]
}