{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "08_dictionary_based_analyses.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNnza/mHf8NE6dKWzZNNRUz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amckenny/text_analytics_intro/blob/main/notebooks/08_dictionary_based_analyses.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ophQRbE7a2dK"
      },
      "source": [
        "# Prerequisites\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kR5t4vxGao5r"
      },
      "source": [
        "# Load 3rd-party packages\n",
        "!pip -q install python-Levenshtein\n",
        "!pip -q install \"gensim==4.0.0\"\n",
        "\n",
        "# Import Standard Library packages\n",
        "import glob\n",
        "from collections import Counter\n",
        "from itertools import compress, takewhile\n",
        "from IPython.display import display\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "# Import 3rd-party packages\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gensim.models import KeyedVectors\n",
        "from nltk import agreement\n",
        "from nltk.corpus import wordnet\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "nltk.download('wordnet', quiet=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-f_2VdXbPmD"
      },
      "source": [
        "# Data Prerequisites\n",
        "!mkdir -p texts\n",
        "\n",
        "from_raw_texts = False # Stanza takes a while to download, loading from raw texts is slower than opening a pre-tokenized dataframe\n",
        "if from_raw_texts:\n",
        "  \n",
        "  # Get text files\n",
        "  !wget -q https://www.dropbox.com/s/5ibk0k4mibcq3q6/AussieTop100private.zip?dl=1 -O ./texts/AussieTop100private.zip\n",
        "  !unzip -qq -n -d ./texts/ ./texts/AussieTop100private.zip\n",
        "\n",
        "  about_dir = Path.cwd() / \"texts\" / \"About\"\n",
        "  pr_dir = Path.cwd() / \"texts\" / \"PR\"\n",
        "  dirs_to_load = [about_dir, pr_dir]\n",
        "\n",
        "\n",
        "  # Text preprocessing prerequisites\n",
        "  !pip install --quiet stanza\n",
        "  import stanza\n",
        "  stanza.download(\"en\")\n",
        "  nlp = stanza.Pipeline('en', processors='tokenize,pos', tokenize_no_ssplit=True)\n",
        "  texts = [] \n",
        "  for directory in dirs_to_load:\n",
        "    for file in glob.glob(f\"{directory}/*.txt\"):\n",
        "      with open(file, 'r') as infile: \n",
        "        text_type = file.split(\"/\")[-2]\n",
        "        text_id = file.split(\"/\")[-1]\n",
        "        fulltext = infile.read().replace(\"\\n\", \" \")\n",
        "        tokens = [word.text.lower() for sentence in nlp(fulltext).sentences for word in sentence.words if word.upos not in [\"PUNCT\", \"SYM\", \"NUM\", 'X']]\n",
        "        texts.append({'text_type': text_type, 'text_id': text_id, 'text': fulltext, 'tokens': tokens})\n",
        "  text_df = pd.DataFrame(texts)\n",
        "\n",
        "else:\n",
        "  # Get pretokenized text dataframe\n",
        "  !wget -q https://www.dropbox.com/s/xg4nuigde974k36/pretokenized_aussie_fbs.pkl?dl=1 -O ./texts/pretokenized_aussie_fbs.pkl\n",
        "  text_df = pd.read_pickle('./texts/pretokenized_aussie_fbs.pkl')\n",
        "  text_df = text_df.drop(['text_no_stops', 'tokens_no_stops'], axis=1)\n",
        "\n",
        "# Get GloVe files and load into gensim\n",
        "!mkdir -p GloVe\n",
        "!wget -q https://www.dropbox.com/s/9k39nheab1rhezq/glove.6B.50d.zip?dl=1 -O ./GloVe/glove.6B.50d.zip\n",
        "!unzip -qq -n -d ./GloVe ./GloVe/glove.6B.50d.zip\n",
        "\n",
        "glove_model = KeyedVectors.load_word2vec_format(\"./GloVe/glove.6B.50d.txt\", binary=False, no_header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CKX2gpbevUX"
      },
      "source": [
        "# Module 8 - Dictionary-Based Computer-Aided Text Analysis\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IupReY4Qe1lo"
      },
      "source": [
        "One of the most basic text analyses is a dictionary-based analysis. With dictionary-based analyses, we provide the computer with a list of words/phrases/stems we believe are associated with a construct of interest (e.g., innovativeness). The computer then examines the frequency with which words associated with that construct are used in each text.\n",
        "\n",
        "The goals for this module are to:\n",
        "* Execute a dictionary-based analysis including words, phrases, and stems as entries\n",
        "* Develop useful functions for dictionary creation and validation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8xF3bM6fIjy"
      },
      "source": [
        "##8.1. Using Existing Dictionaries\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJYdI8Nwfx1u"
      },
      "source": [
        "In previous modules we created a basic dictionary-based analysis function that uses a Counter object to count the number of instances of each token in a text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PNhIRcofAY5"
      },
      "source": [
        "# Counts the number of dictionary words in the passed list of tokens\n",
        "def count_dictionary_words(tokens, dictionary):\n",
        "  word_counter = Counter(tokens)\n",
        "  accumulator = 0\n",
        "  for word in dictionary:\n",
        "    accumulator = accumulator + word_counter.get(word, 0)\n",
        "  return accumulator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HhrssPay-MQ"
      },
      "source": [
        "We can use this function to identify how frequently the texts loaded in the *prerequisites* code use words that we think are associated with 'optimism':"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8VDmUFby-ag"
      },
      "source": [
        "# Creates a new column called 'optimism_words' and uses the count_dictionary_words function to populate that column\n",
        "optimism_words = ['optimism', 'optimistic', 'optimists', 'optimist', 'improve', 'improved', 'improving']\n",
        "text_df['optimism_words'] = text_df['tokens'].apply(lambda x: count_dictionary_words(x, optimism_words))\n",
        "display(text_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q03DVgSLrO5q"
      },
      "source": [
        "This is a good start; however, dictionary-based analyses often include phrases in addition to individual words. Our text preprocessing makes it difficult to identify whether two consecutive tokens are part of a phrase (e.g., two consecutive tokens may be part of different sentences if punctuation has been removed). Fortunately, we can easily count instances of the phrase in the original text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tdol26NrQoD"
      },
      "source": [
        "# Counts the number of times a set of phrases occur in the passed text\n",
        "def count_dictionary_phrases(text, dictionary):\n",
        "  accumulator = 0\n",
        "  for phrase in dictionary:\n",
        "    accumulator = accumulator + text.lower().count(phrase)\n",
        "  return accumulator\n",
        "\n",
        "# Creates a new column called 'optimism_phrases' and uses the count_dictionary_phrases function to populate that column\n",
        "optimism_phrases = ['bank on', 'good omen', 'looking up', 'looks up']\n",
        "text_df['optimism_phrases'] = text_df['text'].apply(lambda x: count_dictionary_phrases(x, optimism_phrases))\n",
        "display(text_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UiWt1uaurZG"
      },
      "source": [
        "Some individuals put word stems in the dictionary to capture all words starting with a particular stem (e.g., hope\\* catches hope, hopeful, hopefully). The dangers of this aside (e.g., hope\\* also catches hopeless, hopelessness), we can extend our word counter to capture word stems.\n",
        "\n",
        "Here instead of returning all Counter entries that match a word in our list, we return all Counter entries that *start* with a stem in our list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tna7N0xIur9H"
      },
      "source": [
        "# Counts the number of times word stems occur the passed tokens\n",
        "def count_dictionary_stems(tokens, dictionary):\n",
        "  word_counter = Counter(tokens)\n",
        "  accumulator = 0\n",
        "  for stem in dictionary:\n",
        "    stem = stem[:-1]\n",
        "    matches = [match for match in word_counter.keys() if match.startswith(stem)]\n",
        "    for match in matches:\n",
        "      accumulator = accumulator + word_counter.get(match, 0)\n",
        "  return accumulator\n",
        "\n",
        "# Creates a new column called 'optimism_stems' and uses the count_dictionary_stems function to populate that column\n",
        "optimism_stems = ['hope*', 'favor*', 'improv*']\n",
        "text_df['optimism_stems'] = text_df['tokens'].apply(lambda x: count_dictionary_stems(x, optimism_stems))\n",
        "display(text_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEwu9hIT0NsC"
      },
      "source": [
        "With the three types of entries (i.e., words, phrases, and stems) counted, we can take the sum of the columns to calculate an overall optimism score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_S035Eke0N_S"
      },
      "source": [
        "# Calculate an overall optimism score\n",
        "text_df['optimism_tot'] = text_df['optimism_words'] + text_df['optimism_phrases'] + text_df['optimism_stems']\n",
        "text_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrL5uZnB1Slr"
      },
      "source": [
        "All that being said, it's a bit of a hassle to have to manually separate the list into words, stems, and phrases. If we can identify rules for each category, we could let Python decide whether each entry is a word, stem, or phrase.\n",
        "\n",
        "Here are our rules:\n",
        "* Phrases contain at least one space in them\n",
        "* Stems end with an asterisk\n",
        "* If it doesn't contain a space or end in an asterisk, it is a word\n",
        "\n",
        "With this insight, we can easily integrate the three functions above into one end-to-end dictionary-based analysis function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yufzUnZH09gM"
      },
      "source": [
        "# Conducts a dictionary analysis agnostic to whether the dictionary contains words/phrases/stems\n",
        "def dictionary_analysis(text, tokens, dictionary):\n",
        "  stems = []\n",
        "  words = []\n",
        "  phrases = []\n",
        "  for entry in dictionary:\n",
        "    if \" \" in entry:\n",
        "      phrases.append(entry)\n",
        "    elif entry.endswith('*'):\n",
        "      stems.append(entry[:-1])\n",
        "    else:\n",
        "      words.append(entry)\n",
        "\n",
        "  word_counter = Counter(tokens)\n",
        "  accumulator = 0\n",
        "\n",
        "  for phrase in phrases:\n",
        "    accumulator = accumulator + text.lower().count(phrase)\n",
        "\n",
        "  for stem in stems:\n",
        "    matches = [match for match in word_counter.keys() if match.startswith(stem)]\n",
        "    for match in matches:\n",
        "      accumulator = accumulator + word_counter.get(match, 0)\n",
        "  \n",
        "  for word in words:\n",
        "    accumulator = accumulator + word_counter.get(word, 0)\n",
        "  \n",
        "  return accumulator\n",
        "\n",
        "optimism_dictionary = ['optimism', 'optimistic', 'optimists', 'optimist', 'improve', 'improved', 'improving', 'hope*', 'favor*', 'improv*', 'bank on', 'good omen', 'looking up', 'looks up']\n",
        "text_df['new_optimism_tot'] = text_df.apply(lambda x: dictionary_analysis(x['text'], x['tokens'], optimism_dictionary), axis=1)\n",
        "text_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CE2noXH2shzi"
      },
      "source": [
        "Looking at the columns for the summed `optimism_tot` column and our newly generated `new_optimism_tot` column, the numbers appear to be consistent. However, it's probably a good idea to check:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJEB0uuI4uVC"
      },
      "source": [
        "same_cols = np.allclose(text_df['optimism_tot'], text_df['new_optimism_tot'])\n",
        "if same_cols:\n",
        "  print(\"The two columns are exactly the same.\")\n",
        "else:\n",
        "  print(\"The two columns are different.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubb1JEkm1S73"
      },
      "source": [
        "While it is possible to use the raw wordcounts in an analysis, we often use *document length normalization* to make documents of varying lengths more directly comparable. To do so, we often simply divide each frequency by the total word count of the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvCZ1LKP0d8D"
      },
      "source": [
        "# Document-length normalize the overall optimism score\n",
        "text_df['dlnorm_optimism'] = text_df['optimism_tot'] / len(text_df['tokens'])\n",
        "text_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aySohxyz57kD"
      },
      "source": [
        "Before we proceed, let's eliminate some of those extra columns that we no longer need."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laVHuGpYw0px"
      },
      "source": [
        "text_df = text_df.drop(['optimism_words', 'optimism_phrases', 'optimism_stems', 'new_optimism_tot'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y00hJ8o46P76"
      },
      "source": [
        "## 8.2. Creating/Refining a Dictionary\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViLG1pvM6eWA"
      },
      "source": [
        "There are many ways to create a dictionary for dictionary-based analysis. A comprehensive review of these approaches is beyond the scope of this module. In this section, we'll largely stick to procedures outlined in two papers: [Short et al. (2010)](https://journals.sagepub.com/doi/abs/10.1177/1094428109335949) and [McKenny et al. (2018)](https://journals.sagepub.com/doi/abs/10.1177/0149206316657594?casa_token=WkDQF900_uEAAAAA:wTc2J5bmpf0LwpE-WnR2iLyZEth261K_JULuzoG46QNi6PAi3fvn8zGXshML3kiafTB1X6WjTuyX). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvazUNuGxKuk"
      },
      "source": [
        "### 8.2.1. Inductive Word List Generation\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caJXlOnRxVAj"
      },
      "source": [
        "In the Short et al. (2010) paper, the authors use DICTION to create an inductively generated list of all words that appear three or more times in at least one text. This can be easily recreated in Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpPmnOYoxCpN"
      },
      "source": [
        "# Identifies all words found frequently in at least one text\n",
        "def inductive_words(tokenized_texts, threshold=3):\n",
        "  word_set = set() # A set is basically a list where there cannot be duplicate entries (as with sets in mathematics)\n",
        "  for text in tokenized_texts:\n",
        "    word_counter = Counter(text).most_common()\n",
        "    word_set = word_set.union({token[0] for token in takewhile(lambda x: x[1] >= threshold, word_counter)})\n",
        "  return word_set\n",
        "\n",
        "threshold = 3\n",
        "inductive_list = inductive_words(text_df['tokens'], threshold)\n",
        "\n",
        "print(f\"Inductive list of {len(inductive_list)} words used {threshold}+ \"\\\n",
        "      f\"times in at least one text from the corpus: {sorted(inductive_list)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqEr5ZvS3k4c"
      },
      "source": [
        "Here we actually expanded our capability, we can set the threshold to whatever we want. Three is the default, but we can change it to 5 if we wanted to.\n",
        "\n",
        "An alternative to the above may be to look at words based on their frequency in the entire corpus rather than within an individual text. We can easily extend our function above to handle either case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30IzqpAv4gY5"
      },
      "source": [
        "# Identifies all words found frequently either in at least one text or in the corpus overall\n",
        "def inductive_words(tokenized_texts, threshold=3, within='text'):\n",
        "  word_set = set() # A set is basically a list where there cannot be duplicate entries (as with sets in mathematics)\n",
        "  if within==\"text\":\n",
        "    for text in tokenized_texts:\n",
        "      word_counter = Counter(text).most_common()\n",
        "      word_set = word_set.union({token[0] for token in takewhile(lambda x: x[1] >= threshold, word_counter)})\n",
        "  elif within==\"corpus\":\n",
        "    corpus_tokens = sum(tokenized_texts, [])\n",
        "    word_counter = Counter(corpus_tokens).most_common()\n",
        "    word_set = {token[0] for token in takewhile(lambda x: x[1] >= threshold, word_counter)}\n",
        "  return word_set\n",
        "\n",
        "threshold = 3\n",
        "within = \"corpus\"\n",
        "inductive_list = inductive_words(text_df['tokens'], threshold, within)\n",
        "\n",
        "print(f\"Inductive list of {len(inductive_list)} words used {threshold}+ \"\\\n",
        "      f\"times (based on {within} frequency): {sorted(inductive_list)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljcVDH0N8uUs"
      },
      "source": [
        "### 8.2.2. Deductive Word List Generation\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E443J7Dw82L2"
      },
      "source": [
        "Short et al. (2010) also create a deductive word list based on the focal construct. They use a thesaurus to generate such words - and there are APIs to thesauruses, such as [Merriam-Webster](https://dictionaryapi.com/products/api-collegiate-thesaurus) that may be used to recreate their procedure directly. However, these APIs require you to request a 'key' making it less feasible for inclusion in this module.\n",
        "\n",
        "However, WordNet, while significantly less comprehensive than a traditional thesaurus, does not require you to apply for a key to use it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxodxNTQA6um"
      },
      "source": [
        "for sense in wordnet.synsets(\"hope\"):\n",
        "  print(f\"{sense.name()} - {sense.definition()}\")\n",
        "  print(f\"{'-'*30}\")\n",
        "  print(f\"Synonyms: {[entry.name() for entry in sense.lemmas()]}\")\n",
        "  print(f\"Hypernyms: {[entry.name() for entry in sense.hypernyms()]}\")\n",
        "  print(f\"Hyponyms: {[entry.name() for entry in sense.hyponyms()]}\")\n",
        "  print(f\"Other words with the same hypernym: {[hypo_entry.name() for hyper_entry in sense.hypernyms() for hypo_entry in hyper_entry.hyponyms() if hypo_entry.name() != sense.name()]}\")\n",
        "  print(f\"\\n{'-'*30}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQhfoGwTA6Ss"
      },
      "source": [
        "Perhaps more useful than that, we saw in the word representations module that there are also other ways of identifying syntactically similar words to a focal word. In particular, we saw that we could use word embeddings, such as GloVe to find similar words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAYjdatv8t-5"
      },
      "source": [
        "glove_model.most_similar(positive=['hope'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVLf3XAsADNQ"
      },
      "source": [
        "Neither of these approaches are likely to be a satisfactory replacement for a comprehensive thesaurus-based search. However, these tools offer valuable complements to such an approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPg1-sc8F4_l"
      },
      "source": [
        "###8.2.3. Dictionary Judging and Interrater Agreement\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsYOxqPBGEgB"
      },
      "source": [
        "After generating an inductive and deductive word list for the construct, researchers generally have raters evaluate the words for inclusion in the final dictionary. To justify inclusion, researchers generally provide evidence of interrater agreement.\n",
        "\n",
        "The NLTK package offers several built-in functions to handle such calculations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHg4eCeNI0vL"
      },
      "source": [
        "words = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
        "coder_1 = [1,0,1,1,0,1,1,0,0,0,0,1,0,1,0,0,0,1,0,0,0,0,1,0,0,1]\n",
        "coder_2 = [1,0,1,1,1,1,0,0,0,0,0,1,0,1,0,1,0,1,0,0,0,0,1,0,0,1]\n",
        "\n",
        "# The format of the codes must be a list of (coder#, word#, their evaluation)\n",
        "formatted_codes = [(1,word_num,coder_1[word_num]) for word_num in range(len(coder_1))] + \\\n",
        "                  [(2,word_num,coder_2[word_num]) for word_num in range(len(coder_2))]\n",
        "\n",
        "ira_annotator = agreement.AnnotationTask(data=formatted_codes)\n",
        "print(f\"Percentage Agreement: {ira_annotator.avg_Ao():.2}\")\n",
        "print(f\"Cohen's Kappa:        {ira_annotator.kappa():.2}\")\n",
        "print(f\"Krippendorff's Alpha: {ira_annotator.alpha():.2}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTqbnZqmMzn3"
      },
      "source": [
        "For simplicity, we had 'two raters' evaluate a small number of individual letters rather than a large number of words; however, we did this to illustrate how you can move from this to compiling a final list.\n",
        "\n",
        "When the authors of a paper aren't the raters, you may not be able to have the raters discuss the disagreed-upon words. In this situation, you have to decide whether to include words where the raters disagreed.\n",
        "\n",
        "Python can handle both:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7Lokr7yOq6Y"
      },
      "source": [
        "include_disagreements = [a | b for a,b in zip(coder_1, coder_2)]\n",
        "print(f\"The liberal (inclusive) word list is:      {list(compress(words, include_disagreements))}\")\n",
        "\n",
        "exclude_disagreements = [a & b for a,b in zip(coder_1, coder_2)]\n",
        "print(f\"The conservative (exclusive) word list is: {list(compress(words, exclude_disagreements))}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y55YeePKQXop"
      },
      "source": [
        "On the other hand, if the authors of the paper are the raters or your raters are willing to discuss differences in coding, you can follow a similar approach to identify the words that the raters disagreed upon:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRitsn10Qt4H"
      },
      "source": [
        "disagreements = [a ^ b for a,b in zip(coder_1, coder_2)]\n",
        "print(f\"The list of words to discuss among the coauthors is: {list(compress(words, disagreements))}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HImenoaeUxiG"
      },
      "source": [
        "###8.2.4. Validity/Reliability Tests\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJSiPkfRVL7T"
      },
      "source": [
        "Short et al. (2010) and McKenny et al. (2018) identify several analyses researchers can do to assess the reliability and validity of a dictionary. Many of these involve examining the correlation of the dictionary measurements with other data. Naturally, this can be done in external statistical tools such as Stata/SAS/SPSS; however, it can also be done within Python directly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSqTGIgcYnoo"
      },
      "source": [
        "# Generate a column based on the optimism column for use in demonstrating correlations\n",
        "# This is for demo purposes only - you would normally use data from another source\n",
        "text_df['comparison_data'] = text_df['optimism_tot'] + np.random.randint(low=0, high=5, size=len(text_df))\n",
        "\n",
        "# Calculate and display the correlations\n",
        "correlation = pearsonr(text_df['optimism_tot'], text_df['comparison_data'])\n",
        "print(f\"The correlation between the optimism dictionary scores and the comparison data is: \"\n",
        "      f\"r = {correlation[0]:.2}; p = {correlation[1]:.2}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}